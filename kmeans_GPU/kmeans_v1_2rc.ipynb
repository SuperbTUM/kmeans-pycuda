{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kmeans_v1.2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FImZ1pKmxjWN",
        "outputId": "3406fdee-c98d-47bf-8a86-5f123871d76b"
      },
      "source": [
        "!pip install pycuda ThrustRTC scikit-cuda"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycuda in /usr/local/lib/python3.7/dist-packages (2021.1)\n",
            "Requirement already satisfied: ThrustRTC in /usr/local/lib/python3.7/dist-packages (0.3.15)\n",
            "Requirement already satisfied: scikit-cuda in /usr/local/lib/python3.7/dist-packages (0.5.3)\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.4.4)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.1.5)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.7/dist-packages (from pycuda) (2021.2.8)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (1.19.5)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from ThrustRTC) (1.14.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (2.0.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->ThrustRTC) (2.20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-5xeYB3a89o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb26a2cb-c8be-4bf8-8ae4-0ef1dd317994"
      },
      "source": [
        "!pip install fsspec"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n",
            "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 40.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 92 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 112 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 122 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 7.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "Successfully installed fsspec-2021.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnrNPbWlmNE_",
        "outputId": "5139d0b6-ac81-45c1-e61c-d6e7d859fa6f"
      },
      "source": [
        "!pip install MulticoreTSNE"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting MulticoreTSNE\n",
            "  Downloading MulticoreTSNE-0.1.tar.gz (20 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from MulticoreTSNE) (1.19.5)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from MulticoreTSNE) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->MulticoreTSNE) (2.20)\n",
            "Building wheels for collected packages: MulticoreTSNE\n",
            "  Building wheel for MulticoreTSNE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for MulticoreTSNE: filename=MulticoreTSNE-0.1-cp37-cp37m-linux_x86_64.whl size=68518 sha256=78d0ca21b7f948d3d402d11bbc0ce1400e272e239ae6afcc5b124702958d11f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/47/df/c0d66e9f775f33281c422a1964de86a59c47f93bb8c37643e3\n",
            "Successfully built MulticoreTSNE\n",
            "Installing collected packages: MulticoreTSNE\n",
            "Successfully installed MulticoreTSNE-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNqgSMNpxpMZ",
        "outputId": "8f8ba334-6de8-46fd-9684-f9be900e307b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7ZiYfAjyEnA",
        "outputId": "9cf51c1d-17c9-41df-beb2-9140560b4344"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "path = \"/content/drive/My Drive\"\n",
        "sys.path.append(path)\n",
        "os.chdir(path)\n",
        "%cd graduation\\ project/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/graduation project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhCCT4JjuI0b",
        "outputId": "05ce431e-a4b5-483d-8d03-80e8078c625a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 14 21:00:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OleEcnNNxjtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aea7e1a-9ec6-492e-eaf2-dc8499e32315"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skcuda import cublas\n",
        "from pycuda import gpuarray\n",
        "import pycuda.autoinit\n",
        "import time\n",
        "from pycuda.compiler import SourceModule\n",
        "import pycuda.driver as cuda\n",
        "import ThrustRTC as trtc\n",
        "import numpy as np\n",
        "import time\n",
        "from matplotlib import pyplot\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "import random\n",
        "import jax.numpy as jnp\n",
        "\n",
        "FLOAT_MAX = 1e10"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/skcuda/cublas.py:284: UserWarning: creating CUBLAS context to get version number\n",
            "  warnings.warn('creating CUBLAS context to get version number')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obMPmIvsyDop"
      },
      "source": [
        "# 求平方\n",
        "get_square = SourceModule(\n",
        "'''\n",
        "__global__ void x_square(const float* __restrict__ x, float *output, int &n)\n",
        "{\n",
        "    const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if(idx<n)\n",
        "        output[idx] = x[idx] * x[idx];\n",
        "}\n",
        "\n",
        "__global__ void y_square(const float* __restrict__ y, float *output, int &m)\n",
        "{\n",
        "    const int idx = threadIdx.x;\n",
        "    if(idx<m)\n",
        "        output[idx] = y[idx] * y[idx];\n",
        "}\n",
        "'''\n",
        ")\n",
        "\n",
        "# 求和\n",
        "square_sum = SourceModule(\n",
        "'''\n",
        "\n",
        "__global__ void sum_row_xy(float *s_x, float *s_y, float *self_sum1, float *self_sum2, int &numSample, int &k, int &dim)\n",
        "{\n",
        "    const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    \n",
        "    if(idx < k)\n",
        "        for(int i=0;i<dim;i++)\n",
        "            self_sum2[idx] += s_y[idx*dim+i];\n",
        "\n",
        "\n",
        "    if(idx < numSample + k && idx >= k)    \n",
        "        for(int j=0;j<dim;j++)    \n",
        "            self_sum1[idx - k] += s_x[(idx - k)*dim+j];    \n",
        "    \n",
        "    __syncthreads(); \n",
        "}\n",
        "\n",
        "\n",
        "__global__ void sum_total(const float* __restrict__ self_sum1, const float* __restrict__ self_sum2, float *result, int &k, int &numSample)\n",
        "{\n",
        "    const int idx1 = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int quo = idx1 % numSample;\n",
        "    int rem = idx1 / numSample;\n",
        "    \n",
        "    if(idx1 < numSample * k)\n",
        "        result[idx1] = self_sum1[quo] + self_sum2[rem];\n",
        "     \n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void sum_row_x(float *s_x, float *self_sum1, int &numSample, int &dim){\n",
        "    const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if(idx < numSample)\n",
        "        for(int j=0;j<dim;j++)    \n",
        "            self_sum1[idx] += s_x[idx*dim+j]; \n",
        "    __syncthreads();\n",
        "}\n",
        "\n",
        "__global__ void sum_row_x_reduction(float *s_x, float *self_sum1, int &numSample, int &dim){\n",
        "    const int idx = blockIdx.x;\n",
        "    int tid = threadIdx.x;  // 42 * 8\n",
        "    __shared__ float s_xdata[1024];\n",
        "    s_xdata[tid] = s_x[tid + dim*idx];\n",
        "    __syncthreads();\n",
        "    for(int i=16; i>0; i >>= 1){\n",
        "      if(tid < i) s_xdata[tid] += s_xdata[tid+i];\n",
        "      __syncthreads();\n",
        "    }\n",
        "    // if (tid < 32) warpReduce(s_xdata, tid);\n",
        "    if(tid == 0){\n",
        "      self_sum1[idx] = s_xdata[0]; \n",
        "      for(int j=32; j < dim; j++)\n",
        "        self_sum1[idx] += s_xdata[j]; \n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__device__ void warpReduce(volatile float* sdata,int tid) {\n",
        "    sdata[tid] += sdata[tid + 32];\n",
        "    sdata[tid] += sdata[tid + 16];\n",
        "    sdata[tid] += sdata[tid + 8];\n",
        "    sdata[tid] += sdata[tid + 4];\n",
        "    sdata[tid] += sdata[tid + 2];\n",
        "    sdata[tid] += sdata[tid + 1];\n",
        "}\n",
        "\n",
        "__global__ void sum_row_y_reduction(float *s_y, float *self_sum2, int &dim){\n",
        "    const int idx = blockIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "    __shared__ float s_ydata[1024];\n",
        "    s_ydata[tid] = s_y[tid + dim*idx];\n",
        "    __syncthreads();\n",
        "    for(int i=16; i>0; i >>= 1){\n",
        "      if(tid < i) s_ydata[tid] += s_ydata[tid+i];\n",
        "      __syncthreads();\n",
        "    }\n",
        "    // if (tid < 16) warpReduce(s_ydata, tid);\n",
        "    if(tid == 0){\n",
        "      self_sum2[idx] = s_ydata[0]; \n",
        "      for(int j=32; j < dim; j++)\n",
        "        self_sum2[idx] += s_ydata[j]; \n",
        "    }\n",
        "    \n",
        "    \n",
        "}\n",
        "\n",
        "'''\n",
        ")\n",
        "\n",
        "\n",
        "# shared_memory.max()=48KB(12000 int/float32)\n",
        "find = SourceModule(\n",
        "'''\n",
        "__global__ void find_range(float *cluster, float *result)\n",
        "{\n",
        "    __shared__ float sdata[512];\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x*blockDim.x+ threadIdx.x;\n",
        "    sdata[tid] = cluster[i];\n",
        "    if(sdata[tid] != sdata[tid+1] and tid < 499) // 统计各簇中元素的个数\n",
        "        result[int(sdata[tid])] = i+1;\n",
        "    __syncthreads();\n",
        "}\n",
        "\n",
        "'''\n",
        ")\n",
        "\n",
        "find_minimum = SourceModule(\n",
        "'''\n",
        "__global__ void find_min(float *list, float *result_value, int *result_idx, int &length, int &n)\n",
        "{\n",
        "    __shared__ float sdata[1024];\n",
        "    __shared__ int ssdata[1024];\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;   \n",
        "\n",
        "\n",
        "    if(tid < length-1)\n",
        "    {\n",
        "        sdata[tid] = list[i + tid * length]; // store data\n",
        "        ssdata[tid] = tid; // store index\n",
        "    }\n",
        "    sdata[length - 1] = 10000;\n",
        "    ssdata[length - 1] = 23;  \n",
        "\n",
        "    __syncthreads();\n",
        "    \n",
        "    unsigned int s;\n",
        "    if(tid < length)\n",
        "    {\n",
        "        for(s = (blockDim.x+1)/2; s>0; s>>=1) {\n",
        "            if (tid < s) \n",
        "            {\n",
        "                ssdata[tid] = (sdata[tid] <= sdata[tid + s])? ssdata[tid]:ssdata[tid+s];\n",
        "                __syncthreads();\n",
        "                sdata[tid] = (sdata[tid] <= sdata[tid + s])? sdata[tid]:sdata[tid+s];\n",
        "                __syncthreads();\n",
        "            }\n",
        "\n",
        "            __syncthreads();\n",
        "\n",
        "}\n",
        "        if(s == 0)\n",
        "        {\n",
        "            ssdata[0] = (sdata[0] <= sdata[n-1])? ssdata[0]:ssdata[n-1];\n",
        "            sdata[0] = (sdata[0] <= sdata[n-1])? sdata[0]:sdata[n-1];\n",
        "            __syncthreads();\n",
        "        }    \n",
        "    }\n",
        "       \n",
        "    if(tid == 0)\n",
        "    {\n",
        "        result_idx[blockIdx.x] = ssdata[0];\n",
        "        result_value[blockIdx.x] = sdata[0];     \n",
        "    }\n",
        "}\n",
        "'''\n",
        ")\n",
        "\n",
        "\n",
        "def cluster_assignment_new(C, k, numSample=494021):\n",
        "    lengthC = k * numSample\n",
        "    n = k    \n",
        "    divider = k + 1 if(k % 2) else k        \n",
        "    while(divider % 2 == 0):\n",
        "        n = divider / 2\n",
        "        divider /= 2\n",
        "    n = np.int32(n)\n",
        "    n_gpu = cuda.mem_alloc(n.nbytes)\n",
        "    cuda.memcpy_htod(n_gpu, n)\n",
        "    \n",
        "    length = np.int32(k+1)\n",
        "    length_gpu = cuda.mem_alloc(length.nbytes)\n",
        "    cuda.memcpy_htod(length_gpu, length)\n",
        "    result_value = gpuarray.to_gpu(np.zeros((lengthC // k, ), dtype = np.float32))\n",
        "    result_idx = gpuarray.to_gpu(np.zeros((lengthC // k, ), dtype = np.int32))\n",
        "    # C = gpuarray.to_gpu(C)\n",
        "\n",
        "    find_func_hybrid = find_minimum.get_function(\"find_min\")\n",
        "    find_func_hybrid(C, result_value, result_idx, length_gpu, n_gpu, block=(k, 1, 1),grid=(lengthC // k, 1))\n",
        "    cluster_assignment = np.zeros((len(result_idx),2))\n",
        "    \n",
        "    cluster_assignment[:,0] = result_idx.get()\n",
        "    cluster_assignment[:,1] = result_value.get()\n",
        "    return cluster_assignment"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrayA1cStRlD",
        "outputId": "e5e99f19-3ee5-43e1-c24b-1557771da675"
      },
      "source": [
        "## test\n",
        "s_y = np.ones((23, 41)).astype(np.float32).flatten()\n",
        "s_y[0] = 2\n",
        "s_y[41] = 4\n",
        "s_y[42] = 5\n",
        "s_y[-1] = 3\n",
        "s_y = gpuarray.to_gpu(s_y)\n",
        "sum2 = np.zeros((23, )).astype(np.float32)\n",
        "sum2 = gpuarray.to_gpu(sum2)\n",
        "k = np.int32(23)\n",
        "k_gpu = cuda.mem_alloc(k.nbytes)\n",
        "cuda.memcpy_htod(k_gpu, k)\n",
        "dim = np.int32(41)\n",
        "dim_gpu = cuda.mem_alloc(dim.nbytes)\n",
        "cuda.memcpy_htod(dim_gpu, dim)\n",
        "test = square_sum.get_function(\"sum_row_y_reduction\")\n",
        "# test = square_sum.get_function(\"sum_row_x\")\n",
        "test(s_y, sum2, dim_gpu, block=(41,1,1), grid=(23,1,1))\n",
        "# test(s_y, sum2, k_gpu, dim_gpu, block=(1024,1,1), grid=(1,1,1))\n",
        "sum2"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([42., 48., 41., 41., 41., 41., 41., 41., 41., 41., 41., 41., 41.,\n",
              "       41., 41., 41., 41., 41., 41., 41., 41., 41., 43.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS7pesKZy_eT"
      },
      "source": [
        "# matrix1 is dataset, matrix2 is centroids\n",
        "def get_x_square(matrix1, numSample, dim):\n",
        "    \"\"\"\n",
        "      It can be interpolated that the dataset square need to calculate for only once!\n",
        "    \"\"\"\n",
        "    n = numSample * dim\n",
        "    grid_dim = int(np.ceil(n / 1024))\n",
        "    n = np.int32(n)\n",
        "    n_gpu = cuda.mem_alloc(n.nbytes)\n",
        "    cuda.memcpy_htod(n_gpu, n)\n",
        "\n",
        "    x = matrix1.flatten().astype(np.float32)\n",
        "    output_x = np.empty_like(x)\n",
        "    \n",
        "    x_gpu = gpuarray.to_gpu_async(x)\n",
        "    output_x_gpu = gpuarray.to_gpu_async(output_x)\n",
        "    x_func = get_square.get_function(\"x_square\")\n",
        "    \n",
        "    # we have a flexible block assignment\n",
        "    x_func(x_gpu, output_x_gpu, n_gpu, block=(1024,1,1), grid=(grid_dim,1))\n",
        "    return output_x_gpu # could be used as matrix1 in get_xy_square\n",
        "\n",
        "def get_xy_square(x_gpu, matrix2, cluster_k, numSample, dim):\n",
        "    m = cluster_k * dim\n",
        "    # determining grid size\n",
        "    m = np.int32(m)\n",
        "    m_gpu = cuda.mem_alloc(m.nbytes)\n",
        "    cuda.memcpy_htod(m_gpu, m)\n",
        "    grid_dim = int(np.ceil(m / 1024))\n",
        "\n",
        "    y_gpu = matrix2.astype(np.float32)\n",
        "    output_y = np.zeros(matrix2.shape).astype(np.float32)\n",
        "\n",
        "    # y_gpu = gpuarray.to_gpu(y)\n",
        "    output_y_gpu = gpuarray.to_gpu_async(output_y)\n",
        "    y_func = get_square.get_function(\"y_square\")\n",
        "    \n",
        "    y_func(y_gpu, output_y_gpu, m_gpu, block=(1024,1,1), grid=(grid_dim,1))\n",
        "\n",
        "    self_sum1 = np.zeros((numSample, ), dtype = np.float32)\n",
        "    self_sum2 = np.zeros((cluster_k, ), dtype = np.float32)\n",
        "    \n",
        "    self_sum1_gpu = gpuarray.to_gpu_async(self_sum1)\n",
        "    self_sum2_gpu = gpuarray.to_gpu_async(self_sum2)\n",
        "    \n",
        "    numSample_np = np.int32(numSample)\n",
        "    numSample_gpu = cuda.mem_alloc(numSample_np.nbytes)\n",
        "    cuda.memcpy_htod(numSample_gpu, numSample_np)\n",
        "    \n",
        "    k = np.int32(cluster_k)\n",
        "    k_gpu = cuda.mem_alloc(k.nbytes)\n",
        "    cuda.memcpy_htod(k_gpu, k)\n",
        "    \n",
        "    dim_ = np.int32(dim)\n",
        "    dim_gpu = cuda.mem_alloc(dim_.nbytes)\n",
        "    cuda.memcpy_htod(dim_gpu, dim_)\n",
        "\n",
        "    result = np.zeros((cluster_k*numSample, ), dtype = np.float32)\n",
        "    block_dim_result = len(result) // 1024 + 1\n",
        "    \n",
        "    result_gpu = gpuarray.to_gpu_async(result)\n",
        "   \n",
        "    # optimization v1.0\n",
        "    # sum_func_row = square_sum.get_function('sum_row_xy')\n",
        "\n",
        "    grid_dim_new = int(np.ceil(numSample / 1024))\n",
        "    # sum_func_row(x_gpu, y_gpu, self_sum1_gpu, self_sum2_gpu, numSample_gpu, k_gpu, dim_gpu, block = (1024,1,1), grid = (grid_dim_new,1))\n",
        "    sum_func_y = square_sum.get_function('sum_row_y_reduction')\n",
        "    sum_func_y(output_y_gpu, self_sum2_gpu, dim_gpu, block=(dim, 1, 1), grid=(cluster_k, 1, 1))\n",
        "\n",
        "    sum_func_x = square_sum.get_function('sum_row_x')\n",
        "    sum_func_x(x_gpu, self_sum1_gpu, numSample_gpu, dim_gpu, block=(1024, 1, 1), grid=(grid_dim_new, 1, 1))\n",
        "\n",
        "    sum_func_total = square_sum.get_function(\"sum_total\")\n",
        "    sum_func_total(self_sum1_gpu, self_sum2_gpu, result_gpu, k_gpu, numSample_gpu, block=(1024,1,1), grid=(block_dim_result,1))\n",
        "    \n",
        "    return result_gpu # result_gpu could be used for matrix3 in dis_computation\n",
        "\n",
        "\n",
        "# 计算与质心之间的距离\n",
        "def dis_computation(matrix1, matrix2, matrix3, orishape_matrix1):\n",
        "    \"\"\"matrix 1 is centroids, \n",
        "    matrix 2 is dataset.T, \n",
        "    matrix 3 is result\"\"\"\n",
        "    matrix1 = matrix1.astype(np.float32)\n",
        "    matrix2 = matrix2.astype(np.float32)\n",
        "    \n",
        "    m = orishape_matrix1[0]\n",
        "    n = matrix2.shape[1]\n",
        "    # matrix3 = matrix3.reshape((23, 494021))\n",
        "    \n",
        "    k = orishape_matrix1[1]\n",
        "    B_gpu = matrix1\n",
        "    #the above is matrix B\n",
        "\n",
        "    A_gpu = matrix2\n",
        "    # the above is matrix A\n",
        "    transa = 'n'\n",
        "    transb = 'n'\n",
        "    alpha = -2\n",
        "    beta = 1\n",
        "    # computing matrix\n",
        "    h = cublas.cublasCreate()\n",
        "    cublas.cublasSgemm(h, transa, transb, n, m, k, alpha, A_gpu.gpudata, n, B_gpu.gpudata, k, beta, matrix3.gpudata, n)\n",
        "    cublas.cublasDestroy(h)\n",
        "    \n",
        "    C = matrix3.get()\n",
        "    return C\n",
        "\n",
        "\n",
        "def cluster_assignment(C, k=23, numSample=494021):\n",
        "    if(len(C) == 1):\n",
        "        return np.array([C, ])\n",
        "    C = C.reshape((k, numSample))\n",
        "    # minDist = np.amin(C, axis = 0)\n",
        "    minDistIdx = np.argmin(C, axis = 0)\n",
        "    # cluster_assignment = np.vstack((minDistIdx, minDist)).T\n",
        "    return minDistIdx\n",
        "\n",
        "\n",
        "\n",
        "def sort_by_key_without_count(clusterAssment):\n",
        "    cluster = clusterAssment[:,0].astype(np.float32)\n",
        "    cluster_gpu = trtc.device_vector_from_numpy(cluster)\n",
        "    n = cluster_gpu.size()\n",
        "    index = trtc.device_vector(\"int32_t\", n)\n",
        "    trtc.Sequence(index)\n",
        "    trtc.Sort_By_Key(cluster_gpu, index)\n",
        "    index_cpu = index.to_host()\n",
        "    cluster_cpu = cluster_gpu.to_host()\n",
        "    return index_cpu, cluster_cpu\n",
        "\n",
        "\n",
        "def sort_gpu(index_cpu, cluster_cpu, dataset, k, placement):\n",
        "    cluster1 = cluster_cpu[0:250000].astype(np.float32)\n",
        "    cluster2 = cluster_cpu[250000:-1].astype(np.float32)\n",
        "    result1 = np.zeros((k-1,),dtype=np.float32)\n",
        "    result2 = np.zeros((k-1,),dtype=np.float32)\n",
        "    find_func = find.get_function(\"find_range\")\n",
        "    find_func(cuda.In(cluster1),cuda.Out(result1), block=(500,1,1),grid=(500,1))\n",
        "    find_func(cuda.In(cluster2),cuda.Out(result2), block=(500,1,1),grid=(500,1))\n",
        "    # 取置信值，这里的代码可以优化一下\n",
        "    # 如果数据量控制在25w以内效果最佳\n",
        "    for j in range(len(result2)):\n",
        "        if(result2[j] != 0):\n",
        "            result2[j] += 250000 # 分批处理，第一批计数从0开始，第二批从250000开始\n",
        "    for i in range(len(result2)-1):\n",
        "        if(result2[i] > result2[i+1]): # 消除异常值\n",
        "            result2[i] = 0\n",
        "    starting_points = [0]\n",
        "    for jj in range(len(result2)):\n",
        "        starting_points.append(max(result1[jj], result2[jj]))\n",
        "    starting_points.append(dataset.shape[0]+1)\n",
        "    data_in_cluster = []\n",
        "    start = int(starting_points[placement])\n",
        "    end = int(starting_points[placement+1])\n",
        "    data_in_cluster = dataset[index_cpu[start:end],:]\n",
        "    data_in_cluster = np.array(data_in_cluster)\n",
        "    return data_in_cluster\n",
        "\n",
        "\n",
        "\n",
        "def cluster_assignment_gpu(C):\n",
        "    C = C.astype(np.float32)\n",
        "    length = np.int32(C.shape[1])\n",
        "    length_gpu = cuda.mem_alloc(length.nbytes)\n",
        "    cuda.memcpy_htod(length_gpu, length)\n",
        "    result = np.zeros((C.shape[0],), dtype = np.float32)\n",
        "    # 找到距离最近的质心点\n",
        "    find_func = find.get_function(\"find_minimum\")\n",
        "    find_func(cuda.In(C),cuda.Out(result),length_gpu,block=(C.shape[1],1,1),grid=(C.shape[0],1))\n",
        "    \n",
        "    result_idx = np.argmin(C, axis = 1)    \n",
        "    cluster_assignment = np.zeros((len(result_idx),2))\n",
        "    cluster_assignment[:,0] = result_idx\n",
        "    cluster_assignment[:,1] = result\n",
        "    return cluster_assignment\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDdfrH3DzVCm"
      },
      "source": [
        "import pycuda\n",
        "\n",
        "\n",
        "# cpu的版本\n",
        "def euclDistance(vector1,vector2):\n",
        "    return np.sqrt(sum(np.power(vector2-vector1, 2))) #power计算次方\n",
        "\n",
        "##初始化数据的中心点，k表示聚类中心数\n",
        "##随机生成k个聚类中心\n",
        "def initCentroids(dataset, k):\n",
        "    numSample, dim = dataset.shape\n",
        "    centroids = np.zeros((k,dim))\n",
        "    for i in range(1,k+1):\n",
        "        #index=int(np.random.uniform(0,numSample))#随机生成数\n",
        "        index = int(i*10000)\n",
        "        centroids[i-1,:] = dataset[index,:]\n",
        "    return centroids\n",
        "\n",
        "##kmean算法\n",
        "def kmeans(dataset, k):\n",
        "    pycuda.tools.clear_context_caches()\n",
        "    numSample, dim = dataset.shape\n",
        "    #生成新的两列数组，保存聚类信息\n",
        "    # 第一列表示所属聚类中心，第二列表示与中心的误差\n",
        "    clusterAssment = np.zeros((numSample, ))#这里dtype就默认\n",
        "    clusterChanged = True\n",
        "\n",
        "    ## step1 初始化聚类中心\n",
        "    centroids = initCentroids(dataset, k)\n",
        "    buffer = np.empty((numSample, ))\n",
        "    itr = 0\n",
        "    dataset_gpu = gpuarray.to_gpu_async(dataset.T)\n",
        "    dataset_gpu_square = get_x_square(dataset, numSample, dim)\n",
        "    orishape_matrix1 = centroids.shape\n",
        "    while itr < 28:\n",
        "        itr += 1\n",
        "        count = 0\n",
        "        comparable_list = zip(buffer, clusterAssment)\n",
        "\n",
        "        for val1, val2 in comparable_list:           \n",
        "            if val1 != val2:\n",
        "                count += 1\n",
        "                if(count > numSample // 10000):\n",
        "                    clusterChanged = True\n",
        "                    break\n",
        "            else:\n",
        "                clusterChanged = False\n",
        "        if clusterChanged is False:\n",
        "            break\n",
        "        \n",
        "        buffer = clusterAssment[:]\n",
        "        #二重循环：对所有数据点，与k个聚类中心计算距离\n",
        "        #并保存标签与距离\n",
        "        matrix1 = gpuarray.to_gpu_async(centroids.flatten())\n",
        "        matrix2 = dataset_gpu\n",
        "        matrix3 = get_xy_square(dataset_gpu_square, matrix1, k, numSample, dim)\n",
        "        # print(itr, matrix3)\n",
        "        distance_mat = dis_computation(matrix1, matrix2, matrix3, orishape_matrix1)  # matrix1: dataset, matrix2: centroids.T, matrix3: (dataset.shape[0], centroids.shape[0]) \n",
        "        # print(itr, distance_mat)\n",
        "        clusterAssment = cluster_assignment(distance_mat, k, numSample)\n",
        "        ## step4 循环结束后更新聚类中心\n",
        "        \n",
        "        for i in range(k):\n",
        "            comp = np.nonzero(clusterAssment == i)[0] # 当前状态的聚类情况\n",
        "            comp_buffer = np.nonzero(buffer == i)[0] # 上一状态的聚类情况\n",
        "            if(np.array_equal(comp, comp_buffer)):\n",
        "                # 当前簇内元素没有改变？如果没有改变，下次聚类忽略该簇\n",
        "                continue\n",
        "            pointsInCluster = dataset[comp]\n",
        "            centroids[i, :] = np.mean(pointsInCluster, axis=0)\n",
        "    ##循环结束，返回聚类中心和标签信息\n",
        "    return centroids, clusterAssment"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMcusnOpzfYG",
        "outputId": "88d1255f-471d-4658-c2e1-f068644764d0"
      },
      "source": [
        "import dask.dataframe as dd\n",
        "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "\n",
        "def evaluation(category_pre, category, numSample):\n",
        "    real = Counter(category)\n",
        "    pre = Counter(category_pre)\n",
        "    print(\"Real category is \", real.items())\n",
        "    print(\"Predicted clustering is \", pre.items())\n",
        "    real = real.most_common()\n",
        "    pre = pre.most_common()\n",
        "    for j in range(numSample):\n",
        "        for nn in range(k):\n",
        "            if(category[j] == real[nn][0]):\n",
        "                category[j] = int(pre[nn][0])\n",
        "    ARI = metrics.adjusted_rand_score(category, category_pre)\n",
        "    AMI = metrics.adjusted_mutual_info_score(category, category_pre)\n",
        "    print(\"The ARI value is {:.4f}\".format(ARI))\n",
        "    print(\"The AMI value is {:.4f}\".format(AMI))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ## load data\n",
        "    dataset = dd.read_csv('kdd_pre_final.csv',sep=',')\n",
        "    # 真实的标签\n",
        "    category_real = dataset.loc[:,[\"classification\"]]\n",
        " \n",
        "    dataset=dataset.loc[:,['duration','protocal_type','service','flag','src_bytes','dst_types','land','wrong_fragment','urgent',\n",
        "              'hot','num_falied_logins','logged_in','num_compromised','root_shell','su_attempted','num_root',\n",
        "              'num_file_creations','num_shells','num_access_files','num_outbound_cmds','is_hot_login','is_guest_login',\n",
        "              'count','srv_count','serror_rate','srv_serror_rate','rerror_rate','srv_rerror_rate','same_error_rate',\n",
        "              'diff_error_rate','srv_diff_host_rate','dst_host_count','dst_host_srv_count','dst_host_same_srv_rate',\n",
        "              'dst_host_diff_srv_rate','dst_host_same_src_port_rate','dst_host_diff_src_port_rate',\n",
        "              'dst_host_serror_rate','dst_host_srv_serror_rate','dst_host_rerror_rate','dst_host_srv_rerror_rate','clustering']]\n",
        "    dataset = np.array(dataset)\n",
        "    ##  k表示聚类中心数\n",
        "    k = 23\n",
        "    times = 1\n",
        "    start = time.time()\n",
        "    for _ in range(times):\n",
        "        centroids, clusterAssment = kmeans(dataset, k)\n",
        "    end = time.time()\n",
        "    print('algorithm (for training) average time: %.2f seconds'%((end - start) / times))\n",
        "    \n",
        "    category_real = np.array(category_real)\n",
        "    category = np.array(list(map(lambda x: x[0], category_real)))\n",
        "    category_pre = np.array(clusterAssment, dtype = np.int8)\n",
        "    evaluation(category_pre, category.astype(np.int8), dataset.shape[0])\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "algorithm (for training) average time: 6.18 seconds\n",
            "Real category is  dict_items([(0, 97278), (1, 30), (2, 9), (3, 3), (4, 107201), (5, 280790), (6, 53), (7, 264), (8, 979), (9, 1040), (10, 1247), (11, 21), (12, 8), (13, 2203), (14, 12), (15, 1589), (16, 4), (17, 231), (18, 7), (19, 20), (20, 1020), (21, 2), (22, 10)])\n",
            "Predicted clustering is  dict_items([(13, 37356), (1, 5724), (8, 38200), (2, 9121), (14, 2962), (0, 3325), (7, 5408), (5, 1008), (19, 73), (22, 55), (21, 226463), (15, 34691), (12, 8476), (18, 10444), (9, 116), (4, 974), (3, 22813), (17, 189), (16, 548), (6, 16960), (10, 20754), (11, 48344), (20, 17)])\n",
            "The ARI value is 0.6297\n",
            "The AMI value is 0.6756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5StNxaL8ciDZ"
      },
      "source": [
        "d = dict()\n",
        "for i in range(k):\n",
        "    d[i] = np.nonzero(category_pre == i)[0] # 当前状态的聚类情况\n",
        "tsne = TSNE(n_jobs=4, n_components=2, perplexity=20) \n",
        "dataset_tsne = tsne.fit_transform(dataset) \n",
        "vis_x, vis_y = dataset_tsne[:,0], dataset_tsne[:,1]\n",
        "for key in d:\n",
        "    plt.scatter(vis_x[d[key]], vis_y[d[key]], cmap=plt.cm.get_cmap(\"jet\", k))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}