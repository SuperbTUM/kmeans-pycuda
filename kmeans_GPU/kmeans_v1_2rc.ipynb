{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kmeans_v1.2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FImZ1pKmxjWN",
        "outputId": "e5aba74d-0efe-42e0-cded-e17d43c96479"
      },
      "source": [
        "!pip install pycuda ThrustRTC scikit-cuda"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2021.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 9.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ThrustRTC\n",
            "  Downloading ThrustRTC-0.3.15-py3-none-any.whl (765 kB)\n",
            "\u001b[K     |████████████████████████████████| 765 kB 38.8 MB/s \n",
            "\u001b[?25hCollecting scikit-cuda\n",
            "  Downloading scikit_cuda-0.5.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 35.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.4.4)\n",
            "Collecting pytools>=2011.2\n",
            "  Downloading pytools-2021.2.8.tar.gz (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting mako\n",
            "  Downloading Mako-1.1.5-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (1.19.5)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from ThrustRTC) (1.14.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (2.0.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->ThrustRTC) (2.20)\n",
            "Building wheels for collected packages: pycuda, pytools\n",
            "  Building wheel for pycuda (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2021.1-cp37-cp37m-linux_x86_64.whl size=627345 sha256=a92c2e602504431f9edcba060baf44daa2c70925a6eec24f1c6861339bfd9e39\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/ef/49/dc6a5feb8d980b37c83d465ecab24949a6aa19458522a9e001\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2021.2.8-py2.py3-none-any.whl size=60725 sha256=984e8591d0ff6773550483b7a452dd5e81e60cc4e74c501b3eaece9aa2335f05\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/2d/ef/0127a17bafa44971f11d05d0e38d7947144cf9e33313bf12a7\n",
            "Successfully built pycuda pytools\n",
            "Installing collected packages: pytools, mako, pycuda, ThrustRTC, scikit-cuda\n",
            "Successfully installed ThrustRTC-0.3.15 mako-1.1.5 pycuda-2021.1 pytools-2021.2.8 scikit-cuda-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-5xeYB3a89o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a492e180-709d-4b91-fa7b-5d36523e0bae"
      },
      "source": [
        "!pip install fsspec"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fsspec\n",
            "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 18.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 92 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 112 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 122 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 9.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "Successfully installed fsspec-2021.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGLJcmbFgkBE"
      },
      "source": [
        "import jax.numpy as jnp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNqgSMNpxpMZ",
        "outputId": "8736dffe-6a6d-4a44-d765-6b39a3064957"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7ZiYfAjyEnA",
        "outputId": "2160adff-b149-41c0-d19b-82e2571f39bf"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "path = \"/content/drive/My Drive\"\n",
        "sys.path.append(path)\n",
        "os.chdir(path)\n",
        "%cd graduation\\ project/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/graduation project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhCCT4JjuI0b",
        "outputId": "320c0d78-1b25-42ee-8b0c-4defd5310db0"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Oct 11 04:01:24 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OleEcnNNxjtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad20de39-ab48-4bcd-ffc2-80388022c31b"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skcuda import cublas\n",
        "from pycuda import gpuarray\n",
        "import pycuda.autoinit\n",
        "import time\n",
        "from pycuda.compiler import SourceModule\n",
        "import pycuda.driver as cuda\n",
        "import ThrustRTC as trtc\n",
        "import numpy as np\n",
        "import time\n",
        "from matplotlib import pyplot\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "import random\n",
        "import jax.numpy as jnp\n",
        "\n",
        "FLOAT_MAX = 1e10"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/skcuda/cublas.py:284: UserWarning: creating CUBLAS context to get version number\n",
            "  warnings.warn('creating CUBLAS context to get version number')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obMPmIvsyDop"
      },
      "source": [
        "# 求平方\n",
        "get_square = SourceModule(\n",
        "'''\n",
        "__global__ void x_square(float *x, int &n)\n",
        "{\n",
        "    const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if(idx<n)\n",
        "        x[idx] = x[idx] * x[idx];\n",
        "}\n",
        "\n",
        "__global__ void y_square(float *y, int &m)\n",
        "{\n",
        "    const int idx = threadIdx.x;\n",
        "    if(idx<m)\n",
        "        y[idx] = y[idx] * y[idx];\n",
        "\n",
        "}\n",
        "'''\n",
        ")\n",
        "\n",
        "# 求和\n",
        "square_sum = SourceModule(\n",
        "'''\n",
        "\n",
        "__global__ void sum_row_xy(float *s_x, float *s_y, float *self_sum1, float *self_sum2, int &numSample, int &k, int &dim)\n",
        "{\n",
        "    const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    \n",
        "    if(idx < k)\n",
        "        for(int i=0;i<dim;i++)\n",
        "            self_sum2[idx] += s_y[idx*dim+i];\n",
        "\n",
        "\n",
        "    if(idx < numSample + k && idx >= k)    \n",
        "        for(int j=0;j<dim;j++)    \n",
        "            self_sum1[idx - k] += s_x[(idx - k)*dim+j];    \n",
        "    \n",
        "    // __syncthreads(); \n",
        "}\n",
        "\n",
        "\n",
        "__global__ void sum_total(float *self_sum1, float *self_sum2, float *result, int &k, int &numSample)\n",
        "{\n",
        "    const int idx1 = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int quo = idx1 % numSample;\n",
        "    int rem = idx1 / numSample;\n",
        "    \n",
        "    if(idx1 < numSample * k)\n",
        "    \n",
        "        result[idx1] = self_sum1[quo] + self_sum2[rem];\n",
        "\n",
        "    __syncthreads();     \n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void sum_row_x(float *s_x, float *self_sum1, int &numSample, int &dim){\n",
        "    const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if(idx < numSample)\n",
        "        for(int j=0;j<dim;j++)    \n",
        "            self_sum1[idx] += s_x[idx*dim+j]; \n",
        "    __syncthreads();\n",
        "}\n",
        "\n",
        "__global__ void sum_row_x_reduction(float *s_x, float *self_sum1, int &numSample, int &dim){\n",
        "    const int idx = blockIdx.x;\n",
        "    int tid = threadIdx.x;  // 42 * 8\n",
        "    __shared__ float s_xdata[1024];\n",
        "    s_xdata[tid] = s_x[tid + dim*idx];\n",
        "    __syncthreads();\n",
        "    for(int i=16; i>0; i >>= 1){\n",
        "      if(tid < i) s_xdata[tid] += s_xdata[tid+i];\n",
        "      __syncthreads();\n",
        "    }\n",
        "    // if (tid < 32) warpReduce(s_xdata, tid);\n",
        "    if(tid == 0){\n",
        "      self_sum1[idx] = s_xdata[0]; \n",
        "      for(int j=32; j < dim; j++)\n",
        "        self_sum1[idx] += s_xdata[j]; \n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__device__ void warpReduce(volatile float* sdata,int tid) {\n",
        "    sdata[tid] += sdata[tid + 32];\n",
        "    sdata[tid] += sdata[tid + 16];\n",
        "    sdata[tid] += sdata[tid + 8];\n",
        "    sdata[tid] += sdata[tid + 4];\n",
        "    sdata[tid] += sdata[tid + 2];\n",
        "    sdata[tid] += sdata[tid + 1];\n",
        "}\n",
        "\n",
        "__global__ void sum_row_y_reduction(float *s_y, float *self_sum2, int &dim){\n",
        "    const int idx = blockIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "    __shared__ float s_ydata[1024];\n",
        "    s_ydata[tid] = s_y[tid + dim*idx];\n",
        "    __syncthreads();\n",
        "    for(int i=16; i>0; i >>= 1){\n",
        "      if(tid < i) s_ydata[tid] += s_ydata[tid+i];\n",
        "      __syncthreads();\n",
        "    }\n",
        "    // if (tid < 16) warpReduce(s_ydata, tid);\n",
        "    if(tid == 0){\n",
        "      self_sum2[idx] = s_ydata[0]; \n",
        "      for(int j=32; j < dim; j++)\n",
        "        self_sum2[idx] += s_ydata[j]; \n",
        "    }\n",
        "    \n",
        "    \n",
        "}\n",
        "\n",
        "'''\n",
        ")\n",
        "\n",
        "\n",
        "# shared_memory.max()=48KB(12000 int/float32)\n",
        "find = SourceModule(\n",
        "'''\n",
        "__global__ void find_range(float *cluster, float *result)\n",
        "{\n",
        "    __shared__ float sdata[512];\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x*blockDim.x+ threadIdx.x;\n",
        "    sdata[tid] = cluster[i];\n",
        "    if(sdata[tid] != sdata[tid+1] and tid < 499) // 统计各簇中元素的个数\n",
        "        result[int(sdata[tid])] = i+1;\n",
        "    __syncthreads();\n",
        "}\n",
        "\n",
        "'''\n",
        ")\n",
        "\n",
        "find_minimum = SourceModule(\n",
        "'''\n",
        "__global__ void find_min(float *list, float *result_value, int *result_idx, int &length, int &n)\n",
        "{\n",
        "    __shared__ float sdata[1024];\n",
        "    __shared__ int ssdata[1024];\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;   \n",
        "\n",
        "\n",
        "    if(tid < length-1)\n",
        "    {\n",
        "        sdata[tid] = list[i + tid * length]; // store data\n",
        "        ssdata[tid] = tid; // store index\n",
        "    }\n",
        "    sdata[length - 1] = 10000;\n",
        "    ssdata[length - 1] = 23;  \n",
        "\n",
        "    __syncthreads();\n",
        "    \n",
        "    unsigned int s;\n",
        "    if(tid < length)\n",
        "    {\n",
        "        for(s = (blockDim.x+1)/2; s>0; s>>=1) {\n",
        "            if (tid < s) \n",
        "            {\n",
        "                ssdata[tid] = (sdata[tid] <= sdata[tid + s])? ssdata[tid]:ssdata[tid+s];\n",
        "                __syncthreads();\n",
        "                sdata[tid] = (sdata[tid] <= sdata[tid + s])? sdata[tid]:sdata[tid+s];\n",
        "                __syncthreads();\n",
        "            }\n",
        "\n",
        "            __syncthreads();\n",
        "\n",
        "}\n",
        "        if(s == 0)\n",
        "        {\n",
        "            ssdata[0] = (sdata[0] <= sdata[n-1])? ssdata[0]:ssdata[n-1];\n",
        "            sdata[0] = (sdata[0] <= sdata[n-1])? sdata[0]:sdata[n-1];\n",
        "            __syncthreads();\n",
        "        }    \n",
        "    }\n",
        "       \n",
        "    if(tid == 0)\n",
        "    {\n",
        "        result_idx[blockIdx.x] = ssdata[0];\n",
        "        result_value[blockIdx.x] = sdata[0];     \n",
        "    }\n",
        "}\n",
        "'''\n",
        ")\n",
        "\n",
        "\n",
        "def cluster_assignment_new(C, k, numSample=494021):\n",
        "    lengthC = k * numSample\n",
        "    n = k    \n",
        "    divider = k + 1 if(k % 2) else k        \n",
        "    while(divider % 2 == 0):\n",
        "        n = divider / 2\n",
        "        divider /= 2\n",
        "    n = np.int32(n)\n",
        "    n_gpu = cuda.mem_alloc(n.nbytes)\n",
        "    cuda.memcpy_htod(n_gpu, n)\n",
        "    \n",
        "    length = np.int32(k+1)\n",
        "    length_gpu = cuda.mem_alloc(length.nbytes)\n",
        "    cuda.memcpy_htod(length_gpu, length)\n",
        "    result_value = gpuarray.to_gpu(np.zeros((lengthC // k, ), dtype = np.float32))\n",
        "    result_idx = gpuarray.to_gpu(np.zeros((lengthC // k, ), dtype = np.int32))\n",
        "    # C = gpuarray.to_gpu(C)\n",
        "\n",
        "    find_func_hybrid = find_minimum.get_function(\"find_min\")\n",
        "    find_func_hybrid(C, result_value, result_idx, length_gpu, n_gpu, block=(k, 1, 1),grid=(lengthC // k, 1))\n",
        "    cluster_assignment = np.zeros((len(result_idx),2))\n",
        "    \n",
        "    cluster_assignment[:,0] = result_idx.get()\n",
        "    cluster_assignment[:,1] = result_value.get()\n",
        "    return cluster_assignment"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrayA1cStRlD",
        "outputId": "b74d285c-0da3-4c10-bbbc-d42e01db8bb1"
      },
      "source": [
        "## test\n",
        "s_y = np.ones((23, 42)).astype(np.float32).flatten()\n",
        "s_y[0] = 2\n",
        "s_y[41] = 4\n",
        "s_y[42] = 5\n",
        "s_y[-1] = 3\n",
        "s_y = gpuarray.to_gpu(s_y)\n",
        "sum2 = np.zeros((23, )).astype(np.float32)\n",
        "sum2 = gpuarray.to_gpu(sum2)\n",
        "k = np.int32(23)\n",
        "k_gpu = cuda.mem_alloc(k.nbytes)\n",
        "cuda.memcpy_htod(k_gpu, k)\n",
        "dim = np.int32(42)\n",
        "dim_gpu = cuda.mem_alloc(dim.nbytes)\n",
        "cuda.memcpy_htod(dim_gpu, dim)\n",
        "test = square_sum.get_function(\"sum_row_y_reduction\")\n",
        "# test = square_sum.get_function(\"sum_row_x\")\n",
        "test(s_y, sum2, dim_gpu, block=(42,1,1), grid=(23,1,1))\n",
        "# test(s_y, sum2, k_gpu, dim_gpu, block=(1024,1,1), grid=(1,1,1))\n",
        "sum2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([46., 46., 42., 42., 42., 42., 42., 42., 42., 42., 42., 42., 42.,\n",
              "       42., 42., 42., 42., 42., 42., 42., 42., 42., 44.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS7pesKZy_eT"
      },
      "source": [
        "# matrix1 is dataset, matrix2 is centroids\n",
        "def get_x_square(matrix1, numSample=494021, dim=42):\n",
        "    \"\"\"\n",
        "      It can be interpolated that the dataset square need to calculate for only once!\n",
        "    \"\"\"\n",
        "    n = numSample * dim\n",
        "    grid_dim = int(np.ceil(n / 1024))\n",
        "    n = np.int32(n)\n",
        "    n_gpu = cuda.mem_alloc(n.nbytes)\n",
        "    cuda.memcpy_htod(n_gpu, n)\n",
        "\n",
        "    x = matrix1.flatten().astype(np.float32)\n",
        "    \n",
        "    x_gpu = gpuarray.to_gpu(x)\n",
        "    x_func = get_square.get_function(\"x_square\")\n",
        "    \n",
        "    # we have a flexible block assignment\n",
        "    x_func(x_gpu, n_gpu, block=(1024,1,1), grid=(grid_dim,1))\n",
        "    return x_gpu # could be used as matrix1 in get_xy_square\n",
        "\n",
        "def get_xy_square(x_gpu, matrix2, cluster_k, numSample, dim):\n",
        "    m = cluster_k * dim\n",
        "    # determining grid size\n",
        "    m = np.int32(m)\n",
        "    m_gpu = cuda.mem_alloc(m.nbytes)\n",
        "    cuda.memcpy_htod(m_gpu, m)\n",
        "    grid_dim = int(np.ceil(m / 1024))\n",
        "\n",
        "    y_gpu = matrix2.astype(np.float32)\n",
        "\n",
        "    # y_gpu = gpuarray.to_gpu(y)\n",
        "    y_func = get_square.get_function(\"y_square\")\n",
        "    \n",
        "    y_func(y_gpu, m_gpu, block=(1024,1,1), grid=(grid_dim,1))\n",
        "\n",
        "    self_sum1 = np.zeros((numSample, ), dtype = np.float32)\n",
        "    self_sum2 = np.zeros((cluster_k, ), dtype = np.float32)\n",
        "    \n",
        "    self_sum1_gpu = gpuarray.to_gpu(self_sum1)\n",
        "    self_sum2_gpu = gpuarray.to_gpu(self_sum2)\n",
        "    \n",
        "    numSample_np = np.int32(numSample)\n",
        "    numSample_gpu = cuda.mem_alloc(numSample_np.nbytes)\n",
        "    cuda.memcpy_htod(numSample_gpu, numSample_np)\n",
        "    \n",
        "    k = np.int32(cluster_k)\n",
        "    k_gpu = cuda.mem_alloc(k.nbytes)\n",
        "    cuda.memcpy_htod(k_gpu, k)\n",
        "    \n",
        "    dim_ = np.int32(dim)\n",
        "    dim_gpu = cuda.mem_alloc(dim_.nbytes)\n",
        "    cuda.memcpy_htod(dim_gpu, dim_)\n",
        "\n",
        "    result = np.zeros((cluster_k*numSample, ), dtype = np.float32)\n",
        "    block_dim_result = len(result) // 1024 + 1\n",
        "    \n",
        "    result_gpu = gpuarray.to_gpu(result)\n",
        "   \n",
        "    # optimization v1.0\n",
        "    # sum_func_row = square_sum.get_function('sum_row_xy')\n",
        "\n",
        "    grid_dim_new = int(np.ceil(numSample / 1024))\n",
        "    # sum_func_row(x_gpu, y_gpu, self_sum1_gpu, self_sum2_gpu, numSample_gpu, k_gpu, dim_gpu, block = (1024,1,1), grid = (grid_dim_new,1))\n",
        "    sum_func_y = square_sum.get_function('sum_row_y_reduction')\n",
        "    sum_func_y(y_gpu, self_sum2_gpu, dim_gpu, block=(dim, 1, 1), grid=(cluster_k, 1, 1))\n",
        "\n",
        "    sum_func_x = square_sum.get_function('sum_row_x')\n",
        "    sum_func_x(x_gpu, self_sum1_gpu, numSample_gpu, dim_gpu, block=(1024, 1, 1), grid=(grid_dim_new, 1, 1))\n",
        "\n",
        "    sum_func_total = square_sum.get_function(\"sum_total\")\n",
        "    sum_func_total(self_sum1_gpu, self_sum2_gpu, result_gpu, k_gpu, numSample_gpu, block=(1024,1,1), grid=(block_dim_result,1))\n",
        "    \n",
        "    return result_gpu # result_gpu could be used for matrix3 in dis_computation\n",
        "\n",
        "\n",
        "# 计算与质心之间的距离\n",
        "def dis_computation(matrix1, matrix2, matrix3, orishape_matrix1):\n",
        "    \"\"\"matrix 1 is centroids, \n",
        "    matrix 2 is dataset.T, \n",
        "    matrix 3 is result\"\"\"\n",
        "    matrix1 = matrix1.astype(np.float32)\n",
        "    matrix2 = matrix2.astype(np.float32)\n",
        "    \n",
        "    m = orishape_matrix1[0]\n",
        "    n = matrix2.shape[1]\n",
        "    \n",
        "    k = orishape_matrix1[1]\n",
        "    B_gpu = matrix1\n",
        "    #the above is matrix B\n",
        "\n",
        "    A_gpu = matrix2\n",
        "    # the above is matrix A\n",
        "    transa = 'n'\n",
        "    transb = 'n'\n",
        "    alpha = -2\n",
        "    beta = 1\n",
        "    # computing matrix\n",
        "    h = cublas.cublasCreate()\n",
        "    cublas.cublasSgemm(h, transa, transb, n, m, k, alpha, A_gpu.gpudata, n, B_gpu.gpudata, k, beta, matrix3.gpudata, n)\n",
        "    cublas.cublasDestroy(h)\n",
        "    \n",
        "    C = matrix3.get()\n",
        "    return C\n",
        "\n",
        "\n",
        "def cluster_assignment(C, k=23, numSample=494021):\n",
        "    if(len(C) == 1):\n",
        "        return np.array([C, ])\n",
        "    C = C.reshape((k, numSample))\n",
        "    # minDist = np.amin(C, axis = 0)\n",
        "    minDistIdx = np.argmin(C, axis = 0)\n",
        "    # cluster_assignment = np.vstack((minDistIdx, minDist)).T\n",
        "    return minDistIdx\n",
        "\n",
        "\n",
        "\n",
        "def sort_by_key_without_count(clusterAssment):\n",
        "    cluster = clusterAssment[:,0].astype(np.float32)\n",
        "    cluster_gpu = trtc.device_vector_from_numpy(cluster)\n",
        "    n = cluster_gpu.size()\n",
        "    index = trtc.device_vector(\"int32_t\", n)\n",
        "    trtc.Sequence(index)\n",
        "    trtc.Sort_By_Key(cluster_gpu, index)\n",
        "    index_cpu = index.to_host()\n",
        "    cluster_cpu = cluster_gpu.to_host()\n",
        "    return index_cpu, cluster_cpu\n",
        "\n",
        "\n",
        "def sort_gpu(index_cpu, cluster_cpu, dataset, k, placement):\n",
        "    cluster1 = cluster_cpu[0:250000].astype(np.float32)\n",
        "    cluster2 = cluster_cpu[250000:-1].astype(np.float32)\n",
        "    result1 = np.zeros((k-1,),dtype=np.float32)\n",
        "    result2 = np.zeros((k-1,),dtype=np.float32)\n",
        "    find_func = find.get_function(\"find_range\")\n",
        "    find_func(cuda.In(cluster1),cuda.Out(result1), block=(500,1,1),grid=(500,1))\n",
        "    find_func(cuda.In(cluster2),cuda.Out(result2), block=(500,1,1),grid=(500,1))\n",
        "    # 取置信值，这里的代码可以优化一下\n",
        "    # 如果数据量控制在25w以内效果最佳\n",
        "    for j in range(len(result2)):\n",
        "        if(result2[j] != 0):\n",
        "            result2[j] += 250000 # 分批处理，第一批计数从0开始，第二批从250000开始\n",
        "    for i in range(len(result2)-1):\n",
        "        if(result2[i] > result2[i+1]): # 消除异常值\n",
        "            result2[i] = 0\n",
        "    starting_points = [0]\n",
        "    for jj in range(len(result2)):\n",
        "        starting_points.append(max(result1[jj], result2[jj]))\n",
        "    starting_points.append(dataset.shape[0]+1)\n",
        "    data_in_cluster = []\n",
        "    start = int(starting_points[placement])\n",
        "    end = int(starting_points[placement+1])\n",
        "    data_in_cluster = dataset[index_cpu[start:end],:]\n",
        "    data_in_cluster = np.array(data_in_cluster)\n",
        "    return data_in_cluster\n",
        "\n",
        "\n",
        "\n",
        "def cluster_assignment_gpu(C):\n",
        "    C = C.astype(np.float32)\n",
        "    length = np.int32(C.shape[1])\n",
        "    length_gpu = cuda.mem_alloc(length.nbytes)\n",
        "    cuda.memcpy_htod(length_gpu, length)\n",
        "    result = np.zeros((C.shape[0],), dtype = np.float32)\n",
        "    # 找到距离最近的质心点\n",
        "    find_func = find.get_function(\"find_minimum\")\n",
        "    find_func(cuda.In(C),cuda.Out(result),length_gpu,block=(C.shape[1],1,1),grid=(C.shape[0],1))\n",
        "    \n",
        "    result_idx = np.argmin(C, axis = 1)    \n",
        "    cluster_assignment = np.zeros((len(result_idx),2))\n",
        "    cluster_assignment[:,0] = result_idx\n",
        "    cluster_assignment[:,1] = result\n",
        "    return cluster_assignment\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDdfrH3DzVCm"
      },
      "source": [
        "import pycuda\n",
        "\n",
        "\n",
        "# cpu的版本\n",
        "def euclDistance(vector1,vector2):\n",
        "    return np.sqrt(sum(np.power(vector2-vector1, 2))) #power计算次方\n",
        "\n",
        "##初始化数据的中心点，k表示聚类中心数\n",
        "##随机生成k个聚类中心\n",
        "def initCentroids(dataset, k):\n",
        "    numSample,dim = dataset.shape\n",
        "    centroids = np.zeros((k,dim))\n",
        "    for i in range(1,k+1):\n",
        "        #index=int(np.random.uniform(0,numSample))#随机生成数\n",
        "        index = int(i*10000)\n",
        "        centroids[i-1,:] = dataset[index,:]\n",
        "    return centroids\n",
        "\n",
        "##kmean算法\n",
        "def kmeans(dataset, k):\n",
        "    pycuda.tools.clear_context_caches()\n",
        "    numSample, dim = dataset.shape\n",
        "    #生成新的两列数组，保存聚类信息\n",
        "    # 第一列表示所属聚类中心，第二列表示与中心的误差\n",
        "    clusterAssment = np.zeros((numSample, ))#这里dtype就默认\n",
        "    clusterChanged = True\n",
        "\n",
        "## step1 初始化聚类中心\n",
        "    centroids = initCentroids(dataset, k)\n",
        "    buffer = np.empty((numSample, ))\n",
        "    itr = 0\n",
        "    dataset_gpu = gpuarray.to_gpu(dataset.T)\n",
        "    dataset_gpu_square = get_x_square(dataset)\n",
        "    orishape_matrix1 = centroids.shape\n",
        "\n",
        "    while itr < 28:\n",
        "        itr += 1\n",
        "        count = 0\n",
        "        comparable_list = zip(buffer, clusterAssment)\n",
        "\n",
        "        for val1, val2 in comparable_list:           \n",
        "            if val1 != val2:\n",
        "                count += 1\n",
        "                if(count > numSample // 10000):\n",
        "                    clusterChanged = True\n",
        "                    break\n",
        "            else:\n",
        "                clusterChanged = False\n",
        "        if clusterChanged is False:\n",
        "            break\n",
        "        \n",
        "        buffer = clusterAssment[:]\n",
        "        #二重循环：对所有数据点，与k个聚类中心计算距离\n",
        "        #并保存标签与距离\n",
        "        matrix1 = gpuarray.to_gpu(centroids.flatten())\n",
        "        matrix2 = dataset_gpu\n",
        "        matrix3 = get_xy_square(dataset_gpu_square, matrix1, k, numSample, dim)\n",
        "        distance_mat = dis_computation(matrix1, matrix2, matrix3, orishape_matrix1)  # matrix1: dataset, matrix2: centroids.T, matrix3: (dataset.shape[0], centroids.shape[0]) \n",
        "        clusterAssment = cluster_assignment(distance_mat, k)\n",
        "## step4 循环结束后更新聚类中心\n",
        "        \n",
        "        for i in range(k):\n",
        "            comp = np.nonzero(clusterAssment == i)[0] # 当前状态的聚类情况\n",
        "            comp_buffer = np.nonzero(buffer == i)[0] # 上一状态的聚类情况\n",
        "            if(np.array_equal(comp, comp_buffer)):\n",
        "                 # 当前簇内元素没有改变？如果没有改变，下次聚类忽略该簇\n",
        "                continue\n",
        "            pointsInCluster = dataset[comp]\n",
        "            centroids[i, :] = np.mean(pointsInCluster, axis=0)\n",
        "    ##循环结束，返回聚类中心和标签信息\n",
        "    return centroids, clusterAssment"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMcusnOpzfYG",
        "outputId": "7b9abd49-9d88-4129-8cc9-d0189310e44a"
      },
      "source": [
        "import dask.dataframe as dd\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "\n",
        "def evaluation(category_pre, category):\n",
        "    real = Counter(category)\n",
        "    pre = Counter(category_pre)\n",
        "    print(\"Real category is \", real.items())\n",
        "    print(\"Predicted clustering is \", pre.items())\n",
        "    real = real.most_common()\n",
        "    pre = pre.most_common()\n",
        "    for j in range(numSample):\n",
        "        for nn in range(k):\n",
        "            if(category[j] == real[nn][0]):\n",
        "                category[j] = int(pre[nn][0])\n",
        "    ARI = metrics.adjusted_rand_score(category, category_pre)\n",
        "    AMI = metrics.adjusted_mutual_info_score(category, category_pre)\n",
        "    print(\"The ARI value is {:.4f}\".format(ARI))\n",
        "    print(\"The AMI value is {:.4f}\".format(AMI))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ## load data\n",
        "    dataset = dd.read_csv('kdd_pre_final.csv',sep=',')\n",
        "    # 真实的标签\n",
        "    category_real = dataset.loc[:,[\"classification\"]]\n",
        " \n",
        "    dataset = dataset.iloc[:,:-1] \n",
        "    dataset = np.array(dataset)\n",
        "    numSample = dataset.shape[0]\n",
        "    ##  k表示聚类中心数\n",
        "    k = 23\n",
        "    total_time = 0\n",
        "    times = 5\n",
        "    start = time.time()\n",
        "    for _ in range(times):\n",
        "        centroids, clusterAssment = kmeans(dataset, k)\n",
        "    end = time.time()\n",
        "    print('algorithm (for training) average time: %.2f seconds'%((end - start) / times))\n",
        "    \n",
        "    category_real = np.array(category_real)\n",
        "    category = np.array(list(map(lambda x: x[0], category_real)))\n",
        "    category_pre = np.array(clusterAssment, dtype = np.int8)\n",
        "    evaluation(category_pre, category.astype(np.int8))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "algorithm (for training) average time: 5.58 seconds\n",
            "Real category is  dict_items([(0, 97278), (1, 30), (2, 9), (3, 3), (4, 107201), (5, 280790), (6, 53), (7, 264), (8, 979), (9, 1040), (10, 1247), (11, 21), (12, 8), (13, 2203), (14, 12), (15, 1589), (16, 4), (17, 231), (18, 7), (19, 20), (20, 1020), (21, 2), (22, 10)])\n",
            "Predicted clustering is  dict_items([(13, 37371), (1, 5816), (8, 38218), (2, 10479), (6, 43), (7, 5393), (5, 38215), (19, 56), (22, 59), (4, 253494), (21, 7861), (18, 11683), (14, 294), (9, 5704), (17, 1598), (16, 53), (12, 1243), (0, 2316), (3, 5098), (11, 48529), (15, 24), (20, 17), (10, 20457)])\n",
            "The ARI value is 0.3577\n",
            "The AMI value is 0.4884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5StNxaL8ciDZ"
      },
      "source": [
        "d = dict()\n",
        "for i in range(k):\n",
        "    d[i] = np.nonzero(category_pre == i)[0] # 当前状态的聚类情况\n",
        "tsne = TSNE(n_components=2) \n",
        "dataset_tsne = tsne.fit_transform(dataset) \n",
        "vis_x, vis_y = dataset_tsne[:,0], dataset_tsne[:,1]\n",
        "for key in d:\n",
        "    plt.scatter(vis_x[d[key]], vis_y[d[key]], cmap=plt.cm.get_cmap(\"jet\", k))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}