{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skcuda import cublas\n",
    "from pycuda import gpuarray\n",
    "import pycuda.autoinit\n",
    "import time\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.driver as cuda\n",
    "import ThrustRTC as trtc\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "FLOAT_MAX = 1e10\n",
    "# np.random.seed(1)\n",
    "# random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求平方\n",
    "get_square = SourceModule(\n",
    "'''\n",
    "__global__ void x_square(float *x, int &n)\n",
    "{\n",
    "    const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if(idx<n)\n",
    "        x[idx] = x[idx] * x[idx];\n",
    "}\n",
    "\n",
    "__global__ void y_square(float *y, int &m)\n",
    "{\n",
    "    const int idx = threadIdx.x;\n",
    "    if(idx<m)\n",
    "        y[idx] = y[idx] * y[idx];\n",
    "\n",
    "}\n",
    "'''\n",
    ")\n",
    "\n",
    "\n",
    "# 求和\n",
    "square_sum = SourceModule(\n",
    "'''\n",
    "\n",
    "__global__ void sum_row_x(float *s_x, float *self_sum1, int &numSample, int &k, int &dim)\n",
    "{\n",
    "    const int idx1 = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    //const int idx2 = threadIdx.y + blockIdx.y * blockDim.y;\n",
    "\n",
    "    __shared__ float sdata[1024];\n",
    "    const int tid = threadIdx.x;\n",
    "    sdata[tid] = s_x[idx1];\n",
    "    __syncthreads();\n",
    "    // do reduction in shared mem\n",
    "    for(unsigned int s=blockDim.x/2; s>0; s>>=1) {\n",
    "    if (tid < s) {\n",
    "    sdata[tid] += sdata[tid + s];\n",
    "    }\n",
    "    __syncthreads();\n",
    "    }\n",
    "    // write result for this block to global mem\n",
    "    if(tid == 0)  self_sum1[blockIdx.x] = sdata[0];\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "__global__ void sum_row_y(float *s_y, float *self_sum2, int &numSample, int &k, int &dim)\n",
    "{\n",
    "    //const int idx1 = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    const int idx2 = threadIdx.y + blockIdx.y * blockDim.y;\n",
    "\n",
    "    __shared__ float sdata[1024];\n",
    "    const int tid = threadIdx.y;\n",
    "    sdata[tid] = s_y[idx2];\n",
    "    __syncthreads();\n",
    "    // do reduction in shared mem\n",
    "    for(unsigned int s=blockDim.x/2; s>0; s>>=1) {\n",
    "    if (tid < s) {\n",
    "    sdata[tid] += sdata[tid + s];\n",
    "    }\n",
    "    __syncthreads();\n",
    "    }\n",
    "    // write result for this block to global mem\n",
    "    if(tid == 0)  self_sum2[blockIdx.x] = sdata[0];\n",
    "    \n",
    "}\n",
    "\n",
    "__global__ void sum_row_xy(float *s_x, float *s_y, float *self_sum1, float *self_sum2, int &numSample, int &k, int &dim)\n",
    "{\n",
    "    const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    //const int idy = threadIdx.y + blockIdx.y * blockDim.y;\n",
    "    \n",
    "    \n",
    "    if(idx < k)\n",
    "        for(int i=0;i<dim;i++)\n",
    "            self_sum2[idx] += s_y[idx*dim+i];\n",
    "\n",
    "\n",
    "    if(idx < numSample + k && idx >= k)    \n",
    "        for(int j=0;j<dim;j++)    \n",
    "            self_sum1[idx - k] += s_x[(idx - k)*dim+j];    \n",
    "    \n",
    "    __syncthreads(); \n",
    "}\n",
    "\n",
    "__global__ void sum_total(float *self_sum1, float *self_sum2, float *result, int &k, int &numSample)\n",
    "{\n",
    "    const int idx1 = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    int quo = idx1 / k;\n",
    "    int mod = idx1 % k;\n",
    "    \n",
    "    if(idx1 < numSample * k)\n",
    "    \n",
    "        result[idx1] = self_sum1[quo] + self_sum2[mod];\n",
    "\n",
    "    __syncthreads();     \n",
    "\n",
    "}\n",
    "\n",
    "__global__ void sum_total_new(float *s_x, float *s_y, float *result, int &numSample, int &k, int &dim)\n",
    "{\n",
    "    const int idx = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "    float self_sum1[50000] = {0};\n",
    "    float self_sum2[5] = {0};\n",
    "    if(idx < k)\n",
    "    {\n",
    "        for(int i=0;i<dim;i++)\n",
    "            self_sum2[idx] += s_y[idx*dim+i];\n",
    "    }\n",
    "\n",
    "    if(idx < numSample + k && idx >= k)    \n",
    "        for(int j=0;j<dim;j++)    \n",
    "            self_sum1[idx - k] += s_x[(idx - k)*dim+j];    \n",
    "    \n",
    "    //__syncthreads(); \n",
    "    if(idx >= numSample + k && idx < numSample * 2 + k)\n",
    "        for(int ii=0; ii<k; ii++)\n",
    "        \n",
    "            result[ii + (idx - numSample - k) * k] = self_sum1[idx - numSample - k] + self_sum2[ii];\n",
    "        //flattened\n",
    "        \n",
    "    __syncthreads(); \n",
    "}\n",
    "\n",
    "'''\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 判断距离数据是否为有效数据\n",
    "check = SourceModule(\n",
    "   '''\n",
    "__global__ void check_if_valid(float *g_data, int &length){\n",
    "    float temp_data[512]={0};\n",
    "    \n",
    "    unsigned int tid = threadIdx.x;\n",
    "    unsigned int i= blockIdx.x*blockDim.x+ threadIdx.x;\n",
    "    if(tid < length)\n",
    "    {\n",
    "    temp_data[tid] = g_data[i];\n",
    "    }\n",
    "    __syncthreads();    \n",
    "    if (tid < length)     \n",
    "    g_data[i] = (temp_data[tid] > 0)? temp_data[tid]:0;\n",
    "    \n",
    "    __syncthreads();\n",
    "    }\n",
    "  \n",
    "   '''\n",
    ")\n",
    "\n",
    "\n",
    "# shared_memory.max()=48KB(12000 int/float32)\n",
    "find = SourceModule(\n",
    "'''\n",
    "__global__ void find_range(float *cluster, float *result)\n",
    "{\n",
    "    __shared__ float sdata[512];\n",
    "    unsigned int tid = threadIdx.x;\n",
    "    unsigned int i = blockIdx.x*blockDim.x+ threadIdx.x;\n",
    "    sdata[tid] = cluster[i];\n",
    "    __syncthreads();\n",
    "    if(sdata[tid] != sdata[tid+1] and tid < 499) // 统计各簇中元素的个数\n",
    "        result[int(sdata[tid])] = i+1;\n",
    "    __syncthreads();\n",
    "}\n",
    "\n",
    "\n",
    "// 下面两个函数可以合并成一个函数\n",
    "// min函数在cuda中复现，使用reduction思想\n",
    "__global__ void find_minimum(float *list, float *result_value, int &length)\n",
    "{\n",
    "    __shared__ float sdata[512];\n",
    "    unsigned int tid = threadIdx.x;\n",
    "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;    \n",
    "    if(tid < length)\n",
    "    {\n",
    "    sdata[tid] = list[i];    \n",
    "    }\n",
    "\n",
    "    __syncthreads();\n",
    "    \n",
    "    if(tid < length)\n",
    "        for(unsigned int s=blockDim.x/2; s>0; s>>=1) {\n",
    "        if (tid < s)\n",
    "        {\n",
    "        sdata[tid] = (sdata[tid] < sdata[tid + s])? sdata[tid] : sdata[tid + s];        \n",
    "        }\n",
    "        __syncthreads();\n",
    "        }\n",
    "    if(tid == 0)\n",
    "    result_value[blockIdx.x] = sdata[0]; \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "__global__ void find_minimum_index(float *list, int *result_idx, int &length)\n",
    "{\n",
    "    __shared__ float sdata[512];\n",
    "    __shared__ int ssdata[512];\n",
    "    unsigned int tid = threadIdx.x;\n",
    "    unsigned int i= blockIdx.x*blockDim.x+ threadIdx.x;    \n",
    "    if(tid < length)\n",
    "    {\n",
    "    sdata[tid] = list[i];\n",
    "    ssdata[tid] = tid;\n",
    "    }\n",
    "\n",
    "    __syncthreads();\n",
    "    if(tid < length)\n",
    "        for(unsigned int s=blockDim.x/2; s>0; s>>=1) {\n",
    "        if (tid < s) \n",
    "            ssdata[tid] = (sdata[tid] < sdata[tid + s])? ssdata[tid]:ssdata[tid+s];\n",
    "            __syncthreads();\n",
    "        if(tid < s)\n",
    "            sdata[tid] = (sdata[tid] < sdata[tid + s])? sdata[tid]:sdata[tid+s];    \n",
    "            __syncthreads();\n",
    "        }\n",
    "\n",
    "    if(tid == 0)\n",
    "    result_idx[blockIdx.x] = ssdata[0];\n",
    "}\n",
    "'''\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix1 is dataset, matrix2 is centroids\n",
    "#数据点得分批去输\n",
    "def get_xy_square(matrix1, matrix2, cluster_k):\n",
    "    n = matrix1.shape[0]*matrix1.shape[1]\n",
    "    grid_dim = n // 1024 + 1\n",
    "    n = np.int32(n)\n",
    "    n_gpu = cuda.mem_alloc(n.nbytes)\n",
    "    cuda.memcpy_htod(n_gpu, n)\n",
    "\n",
    "    x = matrix1.flatten().astype(np.float32)\n",
    "    #print(x.shape)\n",
    "    x_gpu = cuda.mem_alloc(x.nbytes)\n",
    "    cuda.memcpy_htod(x_gpu, x)\n",
    "    x_func = get_square.get_function(\"x_square\")\n",
    "    \n",
    "    # we have a flexible block assignment\n",
    "    x_func(x_gpu, n_gpu, block=(1024,1,1), grid=(grid_dim,1))\n",
    "    # you can print the variables if you want\n",
    "    # cuda.memcpy_dtoh(x, x_gpu)\n",
    "    # x = np.reshape(x, (matrix1.shape[0], matrix1.shape[1]))\n",
    "    \n",
    "    m = matrix2.shape[0] * matrix2.shape[1]\n",
    "    # determining grid size\n",
    "\n",
    "    grid_dim = m // 1024 + 1\n",
    "\n",
    "    m = np.int32(m)\n",
    "    m_gpu = cuda.mem_alloc(m.nbytes)\n",
    "    cuda.memcpy_htod(m_gpu, m)\n",
    "\n",
    "    y = matrix2.flatten().astype(np.float32)\n",
    "    y_gpu = cuda.mem_alloc(y.nbytes)\n",
    "    cuda.memcpy_htod(y_gpu, y)\n",
    "    y_func = get_square.get_function(\"y_square\")\n",
    "    \n",
    "\n",
    "    y_func(y_gpu, m_gpu, block=(1024,1,1), grid=(grid_dim,1))\n",
    "    # you can print the variables if you want \n",
    "    # cuda.memcpy_dtoh(y, y_gpu)\n",
    "    # y = np.reshape(y, (matrix2.shape[0], matrix2.shape[1]))\n",
    "    \n",
    "    self_sum1 = np.zeros((matrix1.shape[0], ), dtype = np.float32)\n",
    "    block_dim = len(self_sum1) // 1024 + 1\n",
    "    self_sum2 = np.zeros((matrix2.shape[0], ), dtype = np.float32)\n",
    "    self_sum1_gpu = cuda.mem_alloc(self_sum1.nbytes)\n",
    "    self_sum2_gpu = cuda.mem_alloc(self_sum2.nbytes)\n",
    "    cuda.memcpy_htod(self_sum1_gpu, self_sum1)\n",
    "    cuda.memcpy_htod(self_sum2_gpu, self_sum2)\n",
    "    \n",
    "    \n",
    "    numSample = np.int32(matrix1.shape[0])\n",
    "    numSample_gpu = cuda.mem_alloc(numSample.nbytes)\n",
    "    cuda.memcpy_htod(numSample_gpu, numSample)\n",
    "    \n",
    "    k = np.int32(cluster_k)\n",
    "    k_gpu = cuda.mem_alloc(k.nbytes)\n",
    "    cuda.memcpy_htod(k_gpu, k)\n",
    "    \n",
    "    dim = np.int32(matrix1.shape[1])\n",
    "    dim_gpu = cuda.mem_alloc(dim.nbytes)\n",
    "    cuda.memcpy_htod(dim_gpu, dim)\n",
    "\n",
    "    result = np.zeros((matrix2.shape[0]*matrix1.shape[0], ), dtype = np.float32)\n",
    "    block_dim_result = len(result) // 1024 + 1\n",
    "    result_gpu = cuda.mem_alloc(result.nbytes)\n",
    "    cuda.memcpy_htod(result_gpu, result)\n",
    "\n",
    "    #sum_func_row_x = square_sum.get_function(\"sum_row_x\")\n",
    "\n",
    "    #sum_func_row_x(x_gpu, self_sum1_gpu, numSample_gpu, k_gpu, dim_gpu, block = (1024, 1, 1), grid = (block_dim,1))\n",
    "\n",
    "    #sum_func_row_y = square_sum.get_function(\"sum_row_y\")\n",
    "   # sum_func_row_y(y_gpu, self_sum2_gpu, numSample_gpu, k_gpu, dim_gpu, block = (1, int(cluster_k + 1), 1), grid = (1,40))\n",
    "    \n",
    "    # optimization v1.0\n",
    "    sum_func_row = square_sum.get_function('sum_row_xy')\n",
    "\n",
    "    grid_dim_new = 50000 // 1024 + 1 # 49\n",
    "    sum_func_row(x_gpu, y_gpu, self_sum1_gpu, self_sum2_gpu, numSample_gpu, k_gpu, dim_gpu, block = (1024,1,1), grid = (grid_dim_new,1))\n",
    "    \n",
    "    sum_func_total = square_sum.get_function(\"sum_total\")\n",
    "    sum_func_total(self_sum1_gpu, self_sum2_gpu, result_gpu, k_gpu, numSample_gpu, block=(1024,1,1), grid=(block_dim_result,1))\n",
    "    \n",
    "    # optimization v2.0??? \n",
    "    #sum_func_total_new = square_sum.get_function(\"sum_total_new\")\n",
    "    #sum_func_total_new(x_gpu, y_gpu, result_gpu, numSample_gpu, k_gpu, dim_gpu, block=(512,1,1),grid=(200,1))\n",
    "    \n",
    "    result_cpu = np.empty_like(result)\n",
    "    cuda.memcpy_dtoh(result_cpu, result_gpu)\n",
    "    result_cpu = np.reshape(result_cpu, (matrix1.shape[0], matrix2.shape[0]))\n",
    "    result_cpu = np.transpose(result_cpu)\n",
    "    \n",
    "    return result_cpu\n",
    "\n",
    "\n",
    "# 计算与质心之间的距离\n",
    "def dis_computation(matrix1, matrix2, matrix3):\n",
    "    \"\"\"matrix 1 is centroids, \n",
    "    matrix 2 is dataset.transpose, \n",
    "    matrix 3 is result\"\"\"\n",
    "    \n",
    "    matrix1 = matrix1.astype(np.float32)\n",
    "    matrix2 = matrix2.astype(np.float32)\n",
    "    matrix3 = matrix3.astype(np.float32)\n",
    "    \n",
    "    ldc = matrix3.shape[1]\n",
    "    m = matrix2.shape[1]\n",
    "    n = matrix1.shape[0]\n",
    "\n",
    "    C = matrix3.flatten()\n",
    "    #the above is matrix C\n",
    "    ldb = matrix1.shape[1]\n",
    "    k = matrix1.shape[1]\n",
    "    B = matrix1.flatten()\n",
    "    #the above is matrix B\n",
    "\n",
    "    lda = matrix2.shape[1]\n",
    "    A = matrix2.flatten()\n",
    "    # the above is matrix A\n",
    "    transa = 'n'\n",
    "    transb = 'n'\n",
    "    alpha = -2\n",
    "    beta = 1\n",
    "    #copy data into GPU\n",
    "    A_gpu = gpuarray.to_gpu(A)\n",
    "    B_gpu = gpuarray.to_gpu(B)\n",
    "    C_gpu = gpuarray.to_gpu(C)\n",
    "    # computing matrix\n",
    "    h = cublas.cublasCreate()\n",
    "    cublas.cublasSgemm(h, transa, transb, m, n, k, alpha, A_gpu.gpudata, lda, B_gpu.gpudata, ldb, beta, C_gpu.gpudata, ldc)\n",
    "    cublas.cublasDestroy(h)\n",
    "    \n",
    "    C = C_gpu.get() # flattened\n",
    "    C = np.reshape(C, (n, m))\n",
    "    \n",
    "    C = C.transpose() # 494021 * 5\n",
    "    C = C.flatten()\n",
    "    return C\n",
    " \n",
    "    \n",
    "\n",
    "def cluster_assignment(C):\n",
    "# 实质上这里也是在排序\n",
    "    minDist = np.amin(C, axis = 1)\n",
    "    minDistIdx = np.argmin(C, axis = 1)\n",
    "    \n",
    "    cluster_assignment = np.zeros((len(minDistIdx),2))\n",
    "    #for i in range(len(minDistIdx)):\n",
    "    cluster_assignment[:,0] = minDistIdx\n",
    "    cluster_assignment[:,1] = minDist\n",
    "    \n",
    "    return cluster_assignment\n",
    "\n",
    "\n",
    "\n",
    "def sort_by_key_without_count(clusterAssment):\n",
    "    cluster = clusterAssment[:,0].astype(np.float32)\n",
    "    cluster_gpu = trtc.device_vector_from_numpy(cluster)\n",
    "    n = cluster_gpu.size()\n",
    "    index = trtc.device_vector(\"int32_t\", n)\n",
    "    trtc.Sequence(index)\n",
    "    trtc.Sort_By_Key(cluster_gpu, index)\n",
    "    index_cpu = index.to_host()\n",
    "    cluster_cpu = cluster_gpu.to_host()\n",
    "    return index_cpu, cluster_cpu\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sort_gpu(index_cpu, cluster_cpu, dataset, k, placement):\n",
    "    if(dataset.shape[0] > 250000):\n",
    "        cluster1 = cluster_cpu[0:250000].astype(np.float32)\n",
    "        cluster2 = cluster_cpu[250000:-1].astype(np.float32)\n",
    "        result1 = np.zeros((k-1,),dtype=np.float32)\n",
    "        result2 = np.zeros((k-1,),dtype=np.float32)\n",
    "        find_func = find.get_function(\"find_range\")\n",
    "        find_func(cuda.In(cluster1),cuda.Out(result1), block=(500,1,1),grid=(500,1))\n",
    "        find_func(cuda.In(cluster2),cuda.Out(result2), block=(500,1,1),grid=(500,1))\n",
    "        # 取置信值，这里的代码可以优化一下\n",
    "        # 如果数据量控制在25w以内效果最佳\n",
    "        for j in range(len(result2)):\n",
    "            if(result2[j] != 0):\n",
    "                result2[j] += 250000 # 分批处理，第一批计数从0开始，第二批从250000开始\n",
    "        for i in range(len(result2)-1):\n",
    "            if(result2[i] > result2[i+1]): # 消除异常值\n",
    "                result2[i] = 0\n",
    "        starting_points = [0]\n",
    "        for jj in range(len(result2)):\n",
    "            starting_points.append(max(result1[jj], result2[jj]))\n",
    "        starting_points.append(dataset.shape[0]+1)\n",
    "        data_in_cluster = []\n",
    "        start = int(starting_points[placement])\n",
    "        end = int(starting_points[placement+1])\n",
    "        data_in_cluster = dataset[index_cpu[start:end],:]\n",
    "        data_in_cluster = np.array(data_in_cluster)\n",
    "    else:\n",
    "        cluster1 = cluster_cpu[0:250000].astype(np.float32)\n",
    "        result1 = np.zeros((k-1,),dtype=np.float32)\n",
    "        find_func = find.get_function(\"find_range\")\n",
    "        find_func(cuda.In(cluster1),cuda.Out(result1), block=(500,1,1),grid=(500,1))\n",
    "        starting_points = [0]\n",
    "        for jj in range(len(result1)):\n",
    "            starting_points.append(result1[jj])\n",
    "        starting_points.append(dataset.shape[0]+1)\n",
    "        data_in_cluster = []\n",
    "        start = int(starting_points[placement])\n",
    "        end = int(starting_points[placement+1])\n",
    "        data_in_cluster = dataset[index_cpu[start:end],:]\n",
    "        data_in_cluster = np.array(data_in_cluster)\n",
    "    return data_in_cluster\n",
    "\n",
    "\n",
    "\n",
    "def cluster_assignment_gpu(C):\n",
    "    C = C.astype(np.float32)\n",
    "    length = np.int32(C.shape[1])\n",
    "    length_gpu = cuda.mem_alloc(length.nbytes)\n",
    "    cuda.memcpy_htod(length_gpu, length)\n",
    "    result = np.zeros((C.shape[0],), dtype = np.float32)\n",
    "    # 找到距离最近的质心点\n",
    "    find_func = find.get_function(\"find_minimum\")\n",
    "    find_func(cuda.In(C),cuda.Out(result),length_gpu,block=(C.shape[1],1,1),grid=(C.shape[0],1))\n",
    "    \n",
    "    result_idx = np.argmin(C, axis = 1)    \n",
    "    cluster_assignment = np.zeros((len(result_idx),2))\n",
    "    cluster_assignment[:,0] = result_idx\n",
    "    cluster_assignment[:,1] = result\n",
    "    return cluster_assignment\n",
    "    \n",
    "    \n",
    "#这个函数的进一步优化我还没有想好\n",
    "def centroids_update(pointsInCluster, k):\n",
    "    if(pointsInCluster.size == 0):\n",
    "        return\n",
    "    dim = pointsInCluster.shape[1]\n",
    "    length = pointsInCluster.shape[0]\n",
    "    sum_cpu = []\n",
    "    for i in range(dim):\n",
    "        p = pointsInCluster.transpose()[i].astype(np.float32)\n",
    "        pointsInCluster_test_gpu = trtc.device_vector_from_numpy(p)\n",
    "        sum_cpu.append(trtc.Reductions.Reduce(pointsInCluster_test_gpu))\n",
    "    sum_cpu = np.array(sum_cpu)\n",
    "\n",
    "    kernel = trtc.Kernel(['sum', 'mean', 'dim','length'],\n",
    "    '''\n",
    "        const int idx = threadIdx.x;\n",
    "        if(idx < dim)\n",
    "            mean[idx] = sum[idx]/length;\n",
    "    ''')\n",
    "\n",
    "    \n",
    "    sum_gpu = trtc.device_vector_from_numpy(sum_cpu)\n",
    "    mean_cpu = np.zeros((dim,), dtype = np.float32)\n",
    "    mean_gpu = trtc.device_vector_from_numpy(mean_cpu)\n",
    "    kernel.launch(1,dim,[sum_gpu, mean_gpu, trtc.DVInt32(dim), trtc.DVInt32(length)])\n",
    "    mean_cpu = mean_gpu.to_host()\n",
    "    return mean_cpu\n",
    "\n",
    "\n",
    "def get_centroids_gpu(points, k):\n",
    "    m, n = np.shape(points)\n",
    "    cluster_centers = np.zeros((k , n))\n",
    "    # 1、随机选择一个样本点为第一个聚类中心\n",
    "    index = np.random.randint(0, m)\n",
    "    cluster_centers[0, ] = np.copy(points[index, ])\n",
    "    # 2、初始化一个距离的序列\n",
    "    #d = [0.0 for _ in range(m)]\n",
    "\n",
    "    for i in range(1, k):\n",
    "        sum_all = 0\n",
    "        #for j in range(m):\n",
    "            # 3、对每一个样本找到最近的聚类中心点\n",
    "        matrix1 = cluster_centers[0:i,]\n",
    "        distance_matrix = []\n",
    "        for pack in range(m // 50000 + 1): # well this is tricky\n",
    "            subset = points[pack*50000:min(m, (pack+1)*50000)]\n",
    "            matrix2 = subset.transpose()\n",
    "            matrix3 = get_xy_square(subset, cluster_centers[0:i,], i)\n",
    "            distance_matrix.append(dis_computation(matrix1, matrix2, matrix3))\n",
    "        distance_matrix = np.array(distance_matrix)\n",
    "        distance_matrix_merge = np.vstack((distance_matrix[0], distance_matrix[1]))\n",
    "        if(len(distance_matrix) > 2):\n",
    "            for ii in range(2,len(distance_matrix)):\n",
    "                distance_matrix_merge = np.vstack((distance_matrix_merge, distance_matrix[ii]))\n",
    "\n",
    "        distance_matrix_merge = distance_matrix_merge.reshape((m, i))           \n",
    "        d = np.amin(distance_matrix_merge, axis = 1)\n",
    "            # 4、将所有的最短距离相加\n",
    "        sum_all = np.sum(d)\n",
    "        # 5、取得sum_all之间的随机值\n",
    "        sum_all *= random.random()\n",
    "        # 6、获得距离最远的样本点作为聚类中心点\n",
    "        for j, di in enumerate(d):\n",
    "            sum_all -= di\n",
    "            if sum_all > 0:\n",
    "                continue\n",
    "            cluster_centers[i] = np.copy(points[j, ])\n",
    "            break\n",
    "    return cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.        , 0.00667381]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_minimum = SourceModule(\n",
    "'''\n",
    "__global__ void find_min(float *list, float *result_value, int *result_idx, int &length, int &n)\n",
    "{\n",
    "    __shared__ float sdata[1024];\n",
    "    __shared__ int ssdata[1024];\n",
    "    unsigned int tid = threadIdx.x;\n",
    "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;   \n",
    "\n",
    "    \n",
    "    if(tid < length)\n",
    "    {\n",
    "    sdata[tid] = list[i]; // store data\n",
    "    ssdata[tid] = tid; // store index\n",
    "    }\n",
    "    unsigned int s;    \n",
    "\n",
    "    __syncthreads();\n",
    "    if(tid < length)\n",
    "    {\n",
    "        for(s = blockDim.x/2; s>0; s>>=1) {\n",
    "        if (tid < s) \n",
    "        {\n",
    "            ssdata[tid] = (sdata[tid] < sdata[tid + s])? ssdata[tid]:ssdata[tid+s];\n",
    "            __syncthreads();\n",
    "            sdata[tid] = (sdata[tid] < sdata[tid + s])? sdata[tid]:sdata[tid+s];\n",
    "            __syncthreads();\n",
    "        }\n",
    "            \n",
    "        __syncthreads();\n",
    "\n",
    "}\n",
    "        if(s == 0)\n",
    "        {\n",
    "            ssdata[0] = (sdata[0] < sdata[n-1])? ssdata[0]:ssdata[n-1];\n",
    "            sdata[0] = (sdata[0] < sdata[n-1])? sdata[0]:sdata[n-1];\n",
    "            __syncthreads();\n",
    "        }    \n",
    "    }\n",
    "       \n",
    "\n",
    "    if(tid == 0)\n",
    "    {\n",
    "    result_idx[blockIdx.x] = ssdata[0];\n",
    "    result_value[blockIdx.x] = sdata[0];     \n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "'''\n",
    ")\n",
    "\n",
    "\n",
    "def cluster_assignment_new(C, k):\n",
    "\n",
    "    C = C.astype(np.float32) # flattened\n",
    "    length = np.int32(k)\n",
    "    length_gpu = cuda.mem_alloc(length.nbytes)\n",
    "    cuda.memcpy_htod(length_gpu, length)\n",
    "    result_value = np.zeros((len(C)//k,),dtype = np.float32)\n",
    "    result_idx = np.zeros((len(C)//k,),dtype = np.int32)\n",
    "    n = k\n",
    "    divider = k\n",
    "    while(not(divider % 2)):\n",
    "        n = divider / 2\n",
    "        divider /= 2\n",
    "    n = np.int32(n)\n",
    "    n_gpu = cuda.mem_alloc(n.nbytes)\n",
    "    cuda.memcpy_htod(n_gpu, n)\n",
    "    find_func_hybrid = find_minimum.get_function(\"find_min\")    \n",
    "    find_func_hybrid(cuda.In(C),cuda.Out(result_value),cuda.Out(result_idx),length_gpu, n_gpu, block=(k,1,1),grid=(len(C)//k,1))\n",
    "    cluster_assignment = np.zeros((len(result_idx),2))\n",
    "    #for i in range(len(minDistIdx)):\n",
    "    cluster_assignment[:,0] = result_idx.astype(np.int32)\n",
    "    cluster_assignment[:,1] = result_value\n",
    "    return cluster_assignment\n",
    "\n",
    "dis_mat = np.loadtxt('distance_mat.csv',delimiter = ',')\n",
    "dis_mat = dis_mat.flatten()\n",
    "cluster_assignment_new(dis_mat, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu的版本\n",
    "def euclDistance(vector1,vector2):\n",
    "    return np.sqrt(sum(np.power(vector2-vector1,2)))#power计算次方\n",
    "\n",
    "##初始化数据的中心点，k表示聚类中心数\n",
    "##随机生成k个聚类中心\n",
    "def initCentroids(dataset,k):\n",
    "    numSample,dim=dataset.shape\n",
    "    centroids=np.zeros((k,dim))\n",
    "    for i in range(k):\n",
    "        #index=int(np.random.uniform(0,numSample))#随机生成数\n",
    "        index = int(i*10000)\n",
    "        centroids[i,:]=dataset[index,:]\n",
    "    return centroids\n",
    "\n",
    "##kmean算法\n",
    "def kmeans(dataset,k):\n",
    "    numSample=dataset.shape[0]\n",
    "    #生成新的两列数组，保存聚类信息\n",
    "    # 第一列表示所属聚类中心，第二列表示与中心的误差\n",
    "    clusterAssment=np.zeros((numSample,2))#这里dtype就默认\n",
    "    clusterChanged=True\n",
    "\n",
    "## step1 初始化聚类中心\n",
    "    centroids=initCentroids(dataset,k)\n",
    "    #centroids=get_centroids_gpu(dataset,k)\n",
    "    storage = np.ones((numSample,))\n",
    "    storage = storage * k\n",
    "    itr = 0\n",
    "    t4centupdate = 0\n",
    "    index_all = [i for i in range(dataset.shape[0])]\n",
    "    omit = [] # 在聚类进程中需要忽略的元素\n",
    "    while (clusterChanged):\n",
    "        itr += 1\n",
    "\n",
    "        #clusterChanged=False\n",
    "        \n",
    "        count = 0\n",
    "        for cnt in range(len(storage)):           \n",
    "            if(storage[cnt] != clusterAssment[cnt,0]):\n",
    "                count += 1\n",
    "                if(count > 10):\n",
    "                    clusterChanged = True\n",
    "                    break\n",
    "            else:\n",
    "                clusterChanged = False\n",
    "\n",
    "        storage = clusterAssment[:,0]\n",
    "        #二重循环：对所有数据点，与k个聚类中心计算距离\n",
    "        #并保存标签与距离\n",
    "        matrix1 = centroids\n",
    "        subset = dataset[0:min(numSample, 50000)]\n",
    "        matrix2 = subset.transpose()\n",
    "        matrix3 = get_xy_square(subset, centroids, k)\n",
    "        distance_mat = dis_computation(matrix1, matrix2, matrix3)\n",
    "        \n",
    "        clusterAssment1 = clusterAssment # buffer\n",
    "        if(numSample > 50000): \n",
    "            for pack in range(1, dataset.shape[0] // 50000 + 1): # well this is tricky\n",
    "                subset = dataset[pack*50000:min(numSample, (pack+1)*50000)]\n",
    "                matrix2 = subset.transpose()\n",
    "                matrix3 = get_xy_square(subset, centroids, k)\n",
    "                \n",
    "                distance_mat = np.hstack((distance_mat, dis_computation(matrix1, matrix2, matrix3)))\n",
    "            #print(distance_mat[0:11])\n",
    "            #distance_matrix = np.vstack((distance_matrix[0],distance_matrix[1],distance_matrix[2],distance_matrix[3],distance_matrix[4],distance_matrix[5],distance_matrix[6],distance_matrix[7]))\n",
    "            #distance_matrix_merge = np.vstack((distance_mat[0], distance_mat[1]))\n",
    "            #if(len(distance_mat) > 2):\n",
    "            #    for i in range(2,len(distance_mat)):\n",
    "            #        distance_matrix_merge = np.vstack((distance_matrix_merge, distance_mat[i]))\n",
    "\n",
    "            #distance_matrix1 = distance_matrix_merge.reshape((dataset.shape[0], k))\n",
    "            #distance_matrix1.flatten()\n",
    "\n",
    "\n",
    "        else:\n",
    "            matrix1 = centroids\n",
    "            matrix2 = dataset.transpose()\n",
    "            matrix3 = get_xy_square(dataset, centroids, k)\n",
    "            distance_matrix = dis_computation(matrix1, matrix2, matrix3)\n",
    "            #clusterAssment = cluster_assignment(distance_matrix)\n",
    "        clusterAssment = cluster_assignment_new(distance_mat, k)\n",
    "        #print(clusterAssment)\n",
    "        #temp_clusterAssment = cluster_assignment_new(distance_matrix)\n",
    "        #np.savetxt('temp_clusterAssment.csv', temp_clusterAssment)\n",
    "        #print((clusterAssment[:,0] == temp_clusterAssment[:,0]).all())\n",
    "        #np.savetxt('clusterAssment.csv',clusterAssment)\n",
    "\n",
    "## step4 循环结束后更新聚类中心\n",
    "        #start = time.time()\n",
    "        #index, cluster, count_dict = sort_by_key(clusterAssment)\n",
    "        #index, cluster = sort_by_key_without_count(clusterAssment)\n",
    "        #collection = []\n",
    "        for i in range(k):\n",
    "            comp = np.nonzero(clusterAssment[:,0] == i)[0] # 当前状态的聚类情况\n",
    "            comp1 = np.nonzero(clusterAssment1[:,0] == i)[0] # 上一状态的聚类情况\n",
    "            if(len(comp) == len(comp1) and (comp == comp1).all()):\n",
    "                #print('In Progress')\n",
    "                #s = time.time()\n",
    "             # 当前簇内元素没有改变？如果没有改变，下次聚类忽略该簇\n",
    "                #if(i != k):\n",
    "                #    k_content = np.nonzero(clusterAssment[:,0] == k)[0]\n",
    "                #    clusterAssment[comp, 0] = np.ones(len(clusterAssment[comp,0]),) * (k-1)\n",
    "                #    clusterAssment[k_content, 0] = np.ones(len(clusterAssment[k_content,0]),) * i\n",
    "\n",
    "                #k = k-1\n",
    "                \n",
    "                #e = time.time()\n",
    "                #print(e-s)\n",
    "                #print(k)\n",
    "                #break\n",
    "                continue\n",
    "                \n",
    "            #pointsInCluster = sort_gpu(index, cluster, dataset, k, i) \n",
    "            points_In_k_Cluster_Label=np.nonzero(clusterAssment[:,0]==i)[0]\n",
    "            pointsInCluster=dataset[points_In_k_Cluster_Label]\n",
    "            centroids[i, :] = np.mean(pointsInCluster, axis=0)\n",
    "            #centroids[i,:] = centroids_update(pointsInCluster, k)\n",
    "        #end = time.time()\n",
    "        #t4centupdate += (end-start)\n",
    "    #print(t4centupdate)\n",
    "        if(itr == 30 or k == 0):            \n",
    "            break\n",
    "    #print(itr)\n",
    "    \n",
    "    ##循环结束，返回聚类中心和标签信息\n",
    "    print(\"Congratulations, cluster complete!\")\n",
    "    return clusterAssment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations, cluster complete!\n",
      "algorithm (for training) total time: 5.280585 秒\n",
      "Counter({2: 391458, 0: 97278, 1: 4107, 4: 1126, 3: 52})\n",
      "Counter({1: 280824, 0: 87084, 3: 84114, 4: 28191, 2: 13808})\n",
      "调整兰德指数为0.4447449491311041\n",
      "归一化互信息指数为0.5101248956364369\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    # start=time.time()\n",
    "    ## load data\n",
    "    dataset=pd.read_csv('kdd_pre_final.csv',sep=',')\n",
    "    # 真实的标签\n",
    "    category_real = dataset.loc[:,[\"clustering\"]]\n",
    " \n",
    "    dataset=dataset.loc[:,['duration','protocal_type','service','flag','src_bytes','dst_types','land','wrong_fragment','urgent',\n",
    "                           'hot','num_falied_logins','logged_in','num_compromised','root_shell','su_attempted','num_root',\n",
    "                           'num_file_creations','num_shells','num_access_files','num_outbound_cmds','is_hot_login','is_guest_login',\n",
    "                           'count','srv_count','serror_rate','srv_serror_rate','rerror_rate','srv_rerror_rate','same_error_rate',\n",
    "                           'diff_error_rate','srv_diff_host_rate','dst_host_count','dst_host_srv_count','dst_host_same_srv_rate',\n",
    "                           'dst_host_diff_srv_rate','dst_host_same_src_port_rate','dst_host_diff_src_port_rate',\n",
    "                           'dst_host_serror_rate','dst_host_srv_serror_rate','dst_host_rerror_rate','dst_host_srv_rerror_rate']]\n",
    "\n",
    "    \n",
    "    dataset=np.array(dataset)\n",
    "    \n",
    "    ##  k表示聚类中心数\n",
    "    k = 5\n",
    "    for _ in range(1):\n",
    "        start=time.time()\n",
    "        clusterAssment=kmeans(dataset,k)\n",
    "\n",
    "        end = time.time()\n",
    "        print('algorithm (for training) total time: %2f 秒'%(end-start))\n",
    "    \n",
    "    category_real = np.array(category_real)\n",
    "    category = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        category.append(category_real[i][0])\n",
    "    category = np.array(category)\n",
    "    category_pre = np.array(clusterAssment[:,0], dtype = np.int32)\n",
    "    real = Counter(category)\n",
    "    pre = Counter(category_pre)\n",
    "    print(real)\n",
    "    print(pre)\n",
    "    real = real.most_common()\n",
    "    pre = pre.most_common()\n",
    "    for j in range(dataset.shape[0]):\n",
    "        for nn in range(k):\n",
    "            if(category[j] == real[nn][0]):\n",
    "                category[j] = int(pre[nn][0])\n",
    "    ARI = metrics.adjusted_rand_score(category, category_pre)\n",
    "    AMI = metrics.adjusted_mutual_info_score(category, category_pre)\n",
    "    print(\"调整兰德指数为\" + str(ARI))\n",
    "    print(\"归一化互信息指数为\" + str(AMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<class 'numpy.float64'>\n",
    "[[0.         5.38674307 2.04028177 1.24462509 2.00796652]\n",
    " [0.00667381 5.34895086 1.93472862 1.22462177 1.85017681]\n",
    " [0.01878834 5.24424219 1.84184837 1.17829323 1.70546722]\n",
    " ...\n",
    " [1.0103941  4.55343056 0.81954193 0.29535484 1.09344769]\n",
    " [0.97317219 4.48998356 0.75338745 0.27575207 0.97581577]\n",
    " [0.95939016 4.34241199 0.87591743 0.1807909  1.14280033]]\n",
    "Congratulations, cluster complete!\n",
    "algorithm (for training) total time: 0.549735 秒\n",
    "Counter({2: 391458, 0: 97278, 1: 4107, 4: 1126, 3: 52})\n",
    "Counter({1: 289299, 0: 99335, 4: 70368, 3: 26636, 2: 8383})\n",
    "调整兰德指数为0.3528364522009577\n",
    "归一化互信息指数为0.31198447749325825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.6293652057647705\n",
    "15\n",
    "Congratulations, cluster complete!\n",
    "algorithm (for training) total time: 7.294799 秒\n",
    "2.304995059967041\n",
    "15\n",
    "Congratulations, cluster complete!\n",
    "algorithm (for training) total time: 6.783495 秒\n",
    "2.3896961212158203\n",
    "15\n",
    "Congratulations, cluster complete!\n",
    "algorithm (for training) total time: 6.852052 秒\n",
    "2.33622407913208\n",
    "15\n",
    "Congratulations, cluster complete!\n",
    "algorithm (for training) total time: 6.742210 秒\n",
    "2.3348276615142822\n",
    "15\n",
    "Congratulations, cluster complete!\n",
    "algorithm (for training) total time: 6.674991 秒\n",
    "2.3230175971984863\n",
    "15\n",
    "Congratulations, cluster complete!\n",
    "algorithm (for training) total time: 6.689172 秒\n",
    "2.330873966217041\n",
    "15\n",
    "Congratulations, cluster complete!\n",
    "algorithm (for training) total time: 6.665528 秒\n",
    "2.329209327697754\n",
    "15\n",
    "Congratulations, cluster complete!\n",
    "algorithm (for training) total time: 6.608633 秒\n",
    "2.3477554321289062\n",
    "15\n",
    "Congratulations, cluster complete!\n",
    "algorithm (for training) total time: 6.694374 秒\n",
    "2.2999649047851562\n",
    "15\n",
    "Congratulations, cluster complete!\n",
    "algorithm (for training) total time: 6.601455 秒\n",
    "Counter({2: 391458, 0: 97278, 1: 4107, 4: 1126, 3: 52})\n",
    "Counter({1: 280824, 0: 87084, 3: 84114, 4: 28191, 2: 13808})\n",
    "调整兰德指数为0.4447449491311041\n",
    "归一化互信息指数为0.5101248956364369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeast代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "#import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import copy\n",
    "#from sklearn.model_selection import train_test_split\n",
    "def data_analysis(dataset):\n",
    "    numSample = dataset.shape[0]\n",
    "    dim = dataset.shape[1]\n",
    "##计算欧式距离\n",
    "def euclDistance(vector1,vector2):\n",
    "    return np.sqrt(sum(np.power(vector2-vector1,2)))#power计算次方\n",
    "\n",
    "##初始化数据的中心点，k表示聚类中心数\n",
    "##随机生成k个聚类中心\n",
    "def initCentroids(dataset,k):\n",
    "    numSample,dim=dataset.shape\n",
    "    centroids=np.zeros((k,dim))\n",
    "    for i in range(k):\n",
    "        #index=int(np.random.uniform(0,numSample))#随机生成数\n",
    "        index = int(i*12)\n",
    "        centroids[i,:]=dataset[index,:]\n",
    "    return centroids\n",
    "\n",
    "##kmean算法\n",
    "def kmeans(dataset,k):\n",
    "    numSample=dataset.shape[0]\n",
    "    #生成新的两列数组，保存聚类信息\n",
    "    # 第一列表示所属聚类中心，第二列表示与中心的误差\n",
    "    clusterAssment=np.zeros((numSample,2))#这里dtype就默认\n",
    "    clusterChanged=True\n",
    "\n",
    "## step1 初始化聚类中心\n",
    "    centroids=initCentroids(dataset,k)\n",
    "    storage = np.zeros((numSample,))\n",
    "    itr = 0\n",
    "    t4centupdate = 0\n",
    "    t4assment = 0\n",
    "    while (clusterChanged):\n",
    "        itr += 1\n",
    "        #print(itr)\n",
    "        if(itr == 30):\n",
    "            \n",
    "            break\n",
    "        clusterChanged=False\n",
    "        \n",
    "        \n",
    "        if((storage == clusterAssment[:,0]).all()):\n",
    "            if((storage == 0).all()):\n",
    "                clusterChanged = True\n",
    "            else:\n",
    "                clusterChanged = False\n",
    "        else:\n",
    "            clusterChanged = True\n",
    "        storage = clusterAssment[:,0]\n",
    "        #二重循环：对所有数据点，与k个聚类中心计算距离\n",
    "        #并保存标签与距离\n",
    "        matrix1 = centroids\n",
    "        matrix2 = dataset.transpose()\n",
    "        matrix3 = get_xy_square(dataset, centroids, k)\n",
    "        distance_matrix = dis_computation(matrix1, matrix2, matrix3)\n",
    "        \n",
    "        #np.savetxt(\"temp.csv\",distance_matrix)\n",
    "        #distance_matrix = np.loadtxt(\"temp.csv\")\n",
    "        start = time.time()\n",
    "        clusterAssment = cluster_assignment(distance_matrix)\n",
    "        end = time.time()\n",
    "        t4assment += (end-start)\n",
    "\n",
    "## step4 循环结束后更新聚类中心\n",
    "        start = time.time()\n",
    "        for j in range(k):\n",
    "            #nonzero返回数组中非零元素的位置,\n",
    "            #eg: clusterAssment[:,0] == j\n",
    "            #array([ True,  True,  True,  True,  True, False])\n",
    "\n",
    "            points_In_k_Cluster_Label=np.nonzero(clusterAssment[:,0]==j)[0]\n",
    "\n",
    "            # pointsInCluster=dataset(np.nonzero(clusterAssment[:,0] == j)[0])\n",
    "            pointsInCluster=dataset[points_In_k_Cluster_Label]\n",
    "            centroids[j, :] = np.mean(pointsInCluster, axis=0)\n",
    "        end = time.time()\n",
    "        t4centupdate += (end-start)\n",
    "    print(\"time for centroids update is \" + str(t4centupdate) + \" seconds.\")\n",
    "    print(\"time for assignment is \" + str(t4assment)+\" seconds.\")\n",
    "    ##循环结束，返回聚类中心和标签信息\n",
    "    print(\"Congratulations, cluster complete!\")\n",
    "    return centroids,clusterAssment\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    start=time.time()\n",
    "    ## load data\n",
    "    dataset=pd.read_csv('yeast.csv',sep=',')\n",
    "    category_real = dataset.loc[:,[\"class_protein_localization\"]]\n",
    "    #dataset=pd.read_csv(\"iris.txt\", sep=' ')\n",
    "    #dataset=dataset.loc[:,[\"Sepal.Length\",\"Sepal.Width\",\"Petal.Length\",\"Petal.Width\"]]\n",
    "    #dataset=dataset.loc[:, [\"region-centroid-col\",\"region-centroid-row\",\"region-pixel-count\",\"short-line-density-5\",\"short-line-density-2\",\"vedge-mean\",\"vegde-sd\",\"hedge-mean\",\"hedge-sd\",\"intensity-mean\",\"rawred-mean\",\"rawblue-mean\",\"rawgreen-mean\",\"exred-mean\",\"exblue-mean\",\"exgreen-mean\",\"value-mean\",\"saturation-mean\",\"hue-mean\"]]\n",
    "    dataset=dataset.loc[:,[\"mcg\",\"gvh\",\"alm\",\"mit\",\"erl\",\"pox\",\"vac\",\"nuc\"]]\n",
    "    #dataset=dataset.loc[1:25,[\"Category 1\",\"Category 2\",\"Category 3\",\"Category 4\",\"Category 5\",\"Category 6\",\"Category 7\",\"Category 8\",\"Category 9\",\"Category 10\",\"Category 11\",\"Category 12\",\"Category 13\",\"Category 14\",\"Category 15\",\"Category 16\",\"Category 17\",\"Category 18\",\"Category 19\",\"Category 20\",\"Category 21\",\"Category 22\",\"Category 23\",\"Category 24\"]]\n",
    "    dataset=np.array(dataset)\n",
    "    #train, test = train_test_split(dataset, test_size = 0.2)\n",
    "    ##  k表示聚类中心数\n",
    "    \n",
    "    k=10\n",
    "    centroids,clusterAssment=kmeans(dataset,k)\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    print('algorithm (for training) total time: %2f 秒'%(end-start))\n",
    "    category_real = np.array(category_real)\n",
    "    category = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        category.append(category_real[i][0])\n",
    "    category = np.array(category)\n",
    "    category_pre = np.array(clusterAssment[:,0], dtype = np.int32)\n",
    "    real = Counter(category)\n",
    "    pre = Counter(category_pre)\n",
    "    print(real)\n",
    "    print(pre)\n",
    "    real = real.most_common()\n",
    "    pre = pre.most_common()\n",
    "    for j in range(dataset.shape[0]):\n",
    "        for nn in range(k):\n",
    "            if(category[j] == real[nn][0]):\n",
    "                category[j] = int(pre[nn][0])\n",
    "    ARI = metrics.adjusted_rand_score(category, category_pre)\n",
    "    AMI = metrics.adjusted_mutual_info_score(category, category_pre)\n",
    "    print(\"调整兰德指数为\" + str(ARI))\n",
    "    print(\"归一化互信息指数为\" + str(AMI))\n",
    "    pyplot.scatter(dataset[:, 0], dataset[:, 1], c=clusterAssment[:,0])\n",
    "    pyplot.scatter(centroids[:, 0], centroids[:, 1], c=\"white\", s=150)\n",
    "    pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
