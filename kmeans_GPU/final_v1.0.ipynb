{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skcuda import cublas\n",
    "from pycuda import gpuarray\n",
    "import pycuda.autoinit\n",
    "import time\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.driver as cuda\n",
    "import ThrustRTC as trtc\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "FLOAT_MAX = 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求平方\n",
    "get_square = SourceModule(\n",
    "'''\n",
    "__global__ void x_square(float *x, int &n)\n",
    "{\n",
    "    const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if(idx<n)\n",
    "        x[idx] = x[idx] * x[idx];\n",
    "}\n",
    "\n",
    "__global__ void y_square(float *y, int &m)\n",
    "{\n",
    "    const int idx = threadIdx.x;\n",
    "    if(idx<m)\n",
    "        y[idx] = y[idx] * y[idx];\n",
    "\n",
    "}\n",
    "'''\n",
    ")\n",
    "\n",
    "# 求和\n",
    "square_sum = SourceModule(\n",
    "'''\n",
    "\n",
    "__global__ void sum_row_xy(float *s_x, float *s_y, float *self_sum1, float *self_sum2, int &numSample, int &k, int &dim)\n",
    "{\n",
    "    const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    \n",
    "    if(idx < k)\n",
    "        for(int i=0;i<dim;i++)\n",
    "            self_sum2[idx] += s_y[idx*dim+i];\n",
    "\n",
    "\n",
    "    if(idx < numSample + k && idx >= k)    \n",
    "        for(int j=0;j<dim;j++)    \n",
    "            self_sum1[idx - k] += s_x[(idx - k)*dim+j];    \n",
    "    \n",
    "    __syncthreads(); \n",
    "}\n",
    "\n",
    "__global__ void sum_total(float *self_sum1, float *self_sum2, float *result, int &k, int &numSample)\n",
    "{\n",
    "    const int idx1 = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    int quo = idx1 % numSample;\n",
    "    int rem = idx1 / numSample;\n",
    "    \n",
    "    if(idx1 < numSample * k)\n",
    "    \n",
    "        result[idx1] = self_sum1[quo] + self_sum2[rem];\n",
    "\n",
    "    __syncthreads();     \n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "'''\n",
    ")\n",
    "\n",
    "\n",
    "# shared_memory.max()=48KB(12000 int/float32)\n",
    "find = SourceModule(\n",
    "'''\n",
    "__global__ void find_range(float *cluster, float *result)\n",
    "{\n",
    "    __shared__ float sdata[512];\n",
    "    unsigned int tid = threadIdx.x;\n",
    "    unsigned int i = blockIdx.x*blockDim.x+ threadIdx.x;\n",
    "    sdata[tid] = cluster[i];\n",
    "    __syncthreads();\n",
    "    if(sdata[tid] != sdata[tid+1] and tid < 499) // 统计各簇中元素的个数\n",
    "        result[int(sdata[tid])] = i+1;\n",
    "    __syncthreads();\n",
    "}\n",
    "\n",
    "'''\n",
    ")\n",
    "\n",
    "find_minimum = SourceModule(\n",
    "'''\n",
    "__global__ void find_min(float *list, float *result_value, int *result_idx, int &length, int &n)\n",
    "{\n",
    "    __shared__ float sdata[1024];\n",
    "    __shared__ int ssdata[1024];\n",
    "    unsigned int tid = threadIdx.x;\n",
    "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;   \n",
    "\n",
    "\n",
    "    if(tid < length-1)\n",
    "    {\n",
    "        sdata[tid] = list[i]; // store data\n",
    "        ssdata[tid] = tid; // store index\n",
    "    }\n",
    "    sdata[length - 1] = 10000;\n",
    "    ssdata[length - 1] = 23;  \n",
    "\n",
    "    __syncthreads();\n",
    "    \n",
    "    unsigned int s;\n",
    "    if(tid < length)\n",
    "    {\n",
    "        for(s = (blockDim.x+1)/2; s>0; s>>=1) {\n",
    "            if (tid < s) \n",
    "            {\n",
    "                ssdata[tid] = (sdata[tid] <= sdata[tid + s])? ssdata[tid]:ssdata[tid+s];\n",
    "                __syncthreads();\n",
    "                sdata[tid] = (sdata[tid] <= sdata[tid + s])? sdata[tid]:sdata[tid+s];\n",
    "                __syncthreads();\n",
    "            }\n",
    "\n",
    "            __syncthreads();\n",
    "\n",
    "}\n",
    "        if(s == 0)\n",
    "        {\n",
    "            ssdata[0] = (sdata[0] <= sdata[n-1])? ssdata[0]:ssdata[n-1];\n",
    "            sdata[0] = (sdata[0] <= sdata[n-1])? sdata[0]:sdata[n-1];\n",
    "            __syncthreads();\n",
    "        }    \n",
    "    }\n",
    "       \n",
    "    if(tid == 0)\n",
    "    {\n",
    "        result_idx[blockIdx.x] = ssdata[0];\n",
    "        result_value[blockIdx.x] = sdata[0];     \n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "'''\n",
    ")\n",
    "\n",
    "\n",
    "def cluster_assignment_new(C, k):\n",
    "\n",
    "    n = k\n",
    "    if(k % 2 == 1):\n",
    "        divider = k + 1\n",
    "    else:\n",
    "        divider = k\n",
    "        \n",
    "    while(not(divider % 2)):\n",
    "        n = divider / 2\n",
    "        divider /= 2\n",
    "    n = np.int32(n)\n",
    "    n_gpu = cuda.mem_alloc(n.nbytes)\n",
    "    cuda.memcpy_htod(n_gpu, n)\n",
    "    \n",
    "    \n",
    "    C = C.astype(np.float32) # flattened\n",
    "    length = np.int32(k+1)\n",
    "    length_gpu = cuda.mem_alloc(length.nbytes)\n",
    "    cuda.memcpy_htod(length_gpu, length)\n",
    "    result_value = np.zeros((len(C)//k,),dtype = np.float32)\n",
    "    result_idx = np.zeros((len(C)//k,),dtype = np.int32)\n",
    "\n",
    "\n",
    "    find_func_hybrid = find_minimum.get_function(\"find_min\")\n",
    "    find_func_hybrid(cuda.In(C),cuda.Out(result_value),cuda.Out(result_idx),length_gpu, n_gpu, block=(k,1,1),grid=(len(C)//k,1))\n",
    "    cluster_assignment = np.zeros((len(result_idx),2))\n",
    "    #for i in range(len(minDistIdx)):\n",
    "    cluster_assignment[:,0] = result_idx.astype(int)\n",
    "    cluster_assignment[:,1] = result_value\n",
    "    return cluster_assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix1 is dataset, matrix2 is centroids\n",
    "#数据点得分批去输\n",
    "def get_xy_square(matrix1, matrix2, cluster_k):\n",
    "    n = matrix1.shape[0]*matrix1.shape[1]\n",
    "    grid_dim = n // 1024 + 1\n",
    "    n = np.int32(n)\n",
    "    n_gpu = cuda.mem_alloc(n.nbytes)\n",
    "    cuda.memcpy_htod(n_gpu, n)\n",
    "\n",
    "    x = matrix1.flatten().astype(np.float32)\n",
    "    #x_gpu = cuda.mem_alloc(x.nbytes)\n",
    "    #cuda.memcpy_htod(x_gpu, x)\n",
    "    x_gpu = gpuarray.to_gpu(x)\n",
    "    x_func = get_square.get_function(\"x_square\")\n",
    "    \n",
    "    # we have a flexible block assignment\n",
    "    x_func(x_gpu, n_gpu, block=(1024,1,1), grid=(grid_dim,1))\n",
    "\n",
    "    \n",
    "    m = matrix2.shape[0] * matrix2.shape[1]\n",
    "    # determining grid size\n",
    "    if(m>1024):\n",
    "        grid_dim = m // 1024 + 1\n",
    "    else:\n",
    "        grid_dim = 1\n",
    "    m = np.int32(m)\n",
    "    m_gpu = cuda.mem_alloc(m.nbytes)\n",
    "    cuda.memcpy_htod(m_gpu, m)\n",
    "\n",
    "    y = matrix2.flatten().astype(np.float32)\n",
    "    #y_gpu = cuda.mem_alloc(y.nbytes)\n",
    "    #cuda.memcpy_htod(y_gpu, y)\n",
    "    y_gpu = gpuarray.to_gpu(y)\n",
    "    y_func = get_square.get_function(\"y_square\")\n",
    "    \n",
    "\n",
    "    y_func(y_gpu, m_gpu, block=(1024,1,1), grid=(grid_dim,1))\n",
    "\n",
    "    \n",
    "    self_sum1 = np.zeros((matrix1.shape[0], ), dtype = np.float32)\n",
    "    block_dim = len(self_sum1) // 1024 + 1\n",
    "    self_sum2 = np.zeros((matrix2.shape[0], ), dtype = np.float32)\n",
    "    #self_sum1_gpu = cuda.mem_alloc(self_sum1.nbytes)\n",
    "    #self_sum2_gpu = cuda.mem_alloc(self_sum2.nbytes)\n",
    "    #cuda.memcpy_htod(self_sum1_gpu, self_sum1)\n",
    "    #cuda.memcpy_htod(self_sum2_gpu, self_sum2)\n",
    "    self_sum1_gpu = gpuarray.to_gpu(self_sum1)\n",
    "    self_sum2_gpu = gpuarray.to_gpu(self_sum2)\n",
    "    \n",
    "    numSample = np.int32(matrix1.shape[0])\n",
    "    numSample_gpu = cuda.mem_alloc(numSample.nbytes)\n",
    "    cuda.memcpy_htod(numSample_gpu, numSample)\n",
    "    \n",
    "    k = np.int32(cluster_k)\n",
    "    k_gpu = cuda.mem_alloc(k.nbytes)\n",
    "    cuda.memcpy_htod(k_gpu, k)\n",
    "    \n",
    "    dim = np.int32(matrix1.shape[1])\n",
    "    dim_gpu = cuda.mem_alloc(dim.nbytes)\n",
    "    cuda.memcpy_htod(dim_gpu, dim)\n",
    "\n",
    "    result = np.zeros((matrix2.shape[0]*matrix1.shape[0], ), dtype = np.float32)\n",
    "    block_dim_result = len(result) // 1024 + 1\n",
    "    #result_gpu = cuda.mem_alloc(result.nbytes)\n",
    "    #cuda.memcpy_htod(result_gpu, result)\n",
    "    result_gpu = gpuarray.to_gpu(result)\n",
    "   \n",
    "    # optimization v1.0\n",
    "    sum_func_row = square_sum.get_function('sum_row_xy')\n",
    "\n",
    "    grid_dim_new = 50000 // 1024 + 1 # 49\n",
    "    sum_func_row(x_gpu, y_gpu, self_sum1_gpu, self_sum2_gpu, numSample_gpu, k_gpu, dim_gpu, block = (1024,1,1), grid = (grid_dim_new,1))\n",
    "    \n",
    "    sum_func_total = square_sum.get_function(\"sum_total\")\n",
    "    sum_func_total(self_sum1_gpu, self_sum2_gpu, result_gpu, k_gpu, numSample_gpu, block=(1024,1,1), grid=(block_dim_result,1))\n",
    "        \n",
    "    #result_cpu = np.empty_like(result)\n",
    "    #cuda.memcpy_dtoh(result_cpu, result_gpu)\n",
    "    #result_cpu = result_gpu.get()\n",
    "    #result_cpu = np.reshape(result_cpu, (matrix1.shape[0], matrix2.shape[0]))\n",
    "    #result_cpu = np.transpose(result_cpu)\n",
    "    \n",
    "    return result_gpu # result_gpu could be used for matrix3 in dis_computation\n",
    "\n",
    "\n",
    "# 计算与质心之间的距离\n",
    "def dis_computation(matrix1, matrix2, matrix3):\n",
    "    \"\"\"matrix 1 is centroids, \n",
    "    matrix 2 is dataset.transpose, \n",
    "    matrix 3 is result\"\"\"\n",
    "    \n",
    "    matrix1 = matrix1.astype(np.float32)\n",
    "    matrix2 = matrix2.astype(np.float32)\n",
    "    #matrix3 = matrix3.astype(np.float32)\n",
    "    \n",
    "    ldc = matrix2.shape[1]\n",
    "    m = matrix2.shape[1]\n",
    "    n = matrix1.shape[0]\n",
    "\n",
    "    #C = matrix3.flatten()\n",
    "    #the above is matrix C\n",
    "    ldb = matrix2.shape[0]\n",
    "    k = matrix2.shape[0]\n",
    "    B = matrix1.flatten()\n",
    "    #the above is matrix B\n",
    "\n",
    "    lda = matrix2.shape[1]\n",
    "    A = matrix2.flatten()\n",
    "    # the above is matrix A\n",
    "    transa = 'n'\n",
    "    transb = 'n'\n",
    "    alpha = -2\n",
    "    beta = 1\n",
    "    #copy data into GPU\n",
    "    A_gpu = gpuarray.to_gpu(A)\n",
    "    B_gpu = gpuarray.to_gpu(B)\n",
    "    #C_gpu = gpuarray.to_gpu(C)\n",
    "    # computing matrix\n",
    "    h = cublas.cublasCreate()\n",
    "    cublas.cublasSgemm(h, transa, transb, m, n, k, alpha, A_gpu.gpudata, lda, B_gpu.gpudata, ldb, beta, matrix3.gpudata, ldc)\n",
    "    cublas.cublasDestroy(h)\n",
    "    \n",
    "    C = matrix3.get()\n",
    "    C = np.reshape(C, (n, m))\n",
    "    C = C.transpose() # 1484 * 10 or 494021 * 5\n",
    "    C = C.flatten()\n",
    "\n",
    "    return C\n",
    "\n",
    "\n",
    "def cluster_assignment(C):\n",
    "# 实质上这里也是在排序\n",
    "    if(len(C) == 1):\n",
    "        return np.array([C,0])\n",
    "    C = C.reshape((494021,23))\n",
    "    minDist = np.amin(C, axis = 1)\n",
    "    minDistIdx = np.argmin(C, axis = 1)\n",
    "    \n",
    "    cluster_assignment = np.zeros((len(minDistIdx),2))\n",
    "    #for i in range(len(minDistIdx)):\n",
    "    cluster_assignment[:,0] = minDistIdx\n",
    "    cluster_assignment[:,1] = minDist\n",
    "    \n",
    "    return cluster_assignment\n",
    "\n",
    "\n",
    "\n",
    "def sort_by_key_without_count(clusterAssment):\n",
    "    cluster = clusterAssment[:,0].astype(np.float32)\n",
    "    cluster_gpu = trtc.device_vector_from_numpy(cluster)\n",
    "    n = cluster_gpu.size()\n",
    "    index = trtc.device_vector(\"int32_t\", n)\n",
    "    trtc.Sequence(index)\n",
    "    trtc.Sort_By_Key(cluster_gpu, index)\n",
    "    index_cpu = index.to_host()\n",
    "    cluster_cpu = cluster_gpu.to_host()\n",
    "    return index_cpu, cluster_cpu\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sort_gpu(index_cpu, cluster_cpu, dataset, k, placement):\n",
    "    cluster1 = cluster_cpu[0:250000].astype(np.float32)\n",
    "    cluster2 = cluster_cpu[250000:-1].astype(np.float32)\n",
    "    result1 = np.zeros((k-1,),dtype=np.float32)\n",
    "    result2 = np.zeros((k-1,),dtype=np.float32)\n",
    "    find_func = find.get_function(\"find_range\")\n",
    "    find_func(cuda.In(cluster1),cuda.Out(result1), block=(500,1,1),grid=(500,1))\n",
    "    find_func(cuda.In(cluster2),cuda.Out(result2), block=(500,1,1),grid=(500,1))\n",
    "    # 取置信值，这里的代码可以优化一下\n",
    "    # 如果数据量控制在25w以内效果最佳\n",
    "    for j in range(len(result2)):\n",
    "        if(result2[j] != 0):\n",
    "            result2[j] += 250000 # 分批处理，第一批计数从0开始，第二批从250000开始\n",
    "    for i in range(len(result2)-1):\n",
    "        if(result2[i] > result2[i+1]): # 消除异常值\n",
    "            result2[i] = 0\n",
    "    starting_points = [0]\n",
    "    for jj in range(len(result2)):\n",
    "        starting_points.append(max(result1[jj], result2[jj]))\n",
    "    starting_points.append(dataset.shape[0]+1)\n",
    "    data_in_cluster = []\n",
    "    start = int(starting_points[placement])\n",
    "    end = int(starting_points[placement+1])\n",
    "    data_in_cluster = dataset[index_cpu[start:end],:]\n",
    "    data_in_cluster = np.array(data_in_cluster)\n",
    "    return data_in_cluster\n",
    "\n",
    "\n",
    "\n",
    "def cluster_assignment_gpu(C):\n",
    "    C = C.astype(np.float32)\n",
    "    length = np.int32(C.shape[1])\n",
    "    length_gpu = cuda.mem_alloc(length.nbytes)\n",
    "    cuda.memcpy_htod(length_gpu, length)\n",
    "    result = np.zeros((C.shape[0],), dtype = np.float32)\n",
    "    # 找到距离最近的质心点\n",
    "    find_func = find.get_function(\"find_minimum\")\n",
    "    find_func(cuda.In(C),cuda.Out(result),length_gpu,block=(C.shape[1],1,1),grid=(C.shape[0],1))\n",
    "    \n",
    "    result_idx = np.argmin(C, axis = 1)    \n",
    "    cluster_assignment = np.zeros((len(result_idx),2))\n",
    "    cluster_assignment[:,0] = result_idx\n",
    "    cluster_assignment[:,1] = result\n",
    "    return cluster_assignment\n",
    "    \n",
    "\n",
    "\n",
    "def get_centroids_gpu(points, k):\n",
    "    m, n = np.shape(points)\n",
    "    cluster_centers = np.zeros((k , n))\n",
    "    # 1、随机选择一个样本点为第一个聚类中心\n",
    "    index = np.random.randint(0, m)\n",
    "    cluster_centers[0, ] = np.copy(points[index, ])\n",
    "    # 2、初始化一个距离的序列\n",
    "    #d = [0.0 for _ in range(m)]\n",
    "\n",
    "    for i in range(1, k):\n",
    "        sum_all = 0\n",
    "        #for j in range(m):\n",
    "            # 3、对每一个样本找到最近的聚类中心点\n",
    "        matrix1 = cluster_centers[0:i,]\n",
    "        distance_matrix = []\n",
    "        for pack in range(m // 50000 + 1): # well this is tricky\n",
    "            subset = points[pack*50000:min(m, (pack+1)*50000)]\n",
    "            matrix2 = subset.transpose()\n",
    "            matrix3 = get_xy_square(subset, cluster_centers[0:i,], i)\n",
    "            distance_matrix.append(dis_computation(matrix1, matrix2, matrix3))\n",
    "        distance_matrix = np.array(distance_matrix)\n",
    "        distance_matrix_merge = np.vstack((distance_matrix[0], distance_matrix[1]))\n",
    "        if(len(distance_matrix) > 2):\n",
    "            for ii in range(2,len(distance_matrix)):\n",
    "                distance_matrix_merge = np.vstack((distance_matrix_merge, distance_matrix[ii]))\n",
    "\n",
    "        distance_matrix_merge = distance_matrix_merge.reshape((m, i))           \n",
    "        d = np.amin(distance_matrix_merge, axis = 1)\n",
    "            # 4、将所有的最短距离相加\n",
    "        sum_all = np.sum(d)\n",
    "        # 5、取得sum_all之间的随机值\n",
    "        sum_all *= random.random()\n",
    "        # 6、获得距离最远的样本点作为聚类中心点\n",
    "        for j, di in enumerate(d):\n",
    "            sum_all -= di\n",
    "            if sum_all > 0:\n",
    "                continue\n",
    "            cluster_centers[i] = np.copy(points[j, ])\n",
    "            break\n",
    "    return cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu的版本\n",
    "def euclDistance(vector1,vector2):\n",
    "    return np.sqrt(sum(np.power(vector2-vector1,2)))#power计算次方\n",
    "\n",
    "##初始化数据的中心点，k表示聚类中心数\n",
    "##随机生成k个聚类中心\n",
    "def initCentroids(dataset,k):\n",
    "    numSample,dim=dataset.shape\n",
    "    centroids=np.zeros((k,dim))\n",
    "    for i in range(1,k+1):\n",
    "        #index=int(np.random.uniform(0,numSample))#随机生成数\n",
    "        index = int(i*10000)\n",
    "        centroids[i-1,:]=dataset[index,:]\n",
    "    return centroids\n",
    "\n",
    "##kmean算法\n",
    "def kmeans(dataset,k):\n",
    "    numSample=dataset.shape[0]\n",
    "    #生成新的两列数组，保存聚类信息\n",
    "    # 第一列表示所属聚类中心，第二列表示与中心的误差\n",
    "    clusterAssment=np.zeros((numSample,2))#这里dtype就默认\n",
    "    clusterChanged=True\n",
    "\n",
    "## step1 初始化聚类中心\n",
    "    centroids=initCentroids(dataset,k)\n",
    "    #centroids=get_centroids_gpu(dataset,k)\n",
    "    storage = np.ones((numSample,))\n",
    "    storage = storage * k\n",
    "    itr = 0\n",
    "    t4centupdate = 0\n",
    "    while (clusterChanged):\n",
    "        itr += 1\n",
    "\n",
    "        #clusterChanged=False\n",
    "        \n",
    "        count = 0\n",
    "        for cnt in range(len(storage)):           \n",
    "            if(storage[cnt] != clusterAssment[cnt,0]):\n",
    "                count += 1\n",
    "                if(count > 10):\n",
    "                    clusterChanged = True\n",
    "                    break\n",
    "            else:\n",
    "                clusterChanged = False\n",
    "        #if((storage == clusterAssment[:,0]).all()):\n",
    "        #    clusterChanged = False\n",
    "        #else:\n",
    "        #    clusterChanged = True\n",
    "        storage = clusterAssment[:,0]\n",
    "        #二重循环：对所有数据点，与k个聚类中心计算距离\n",
    "        #并保存标签与距离\n",
    "        matrix1 = centroids\n",
    "        subset = dataset[0:min(numSample, 50000)]\n",
    "        matrix2 = subset.transpose()\n",
    "        matrix3 = get_xy_square(subset, centroids, k)\n",
    "        distance_mat = dis_computation(matrix1, matrix2, matrix3)\n",
    "        \n",
    "        clusterAssment1 = clusterAssment # buffer\n",
    "        if(numSample > 50000): \n",
    "            for pack in range(1, dataset.shape[0] // 50000 + 1): # well this is tricky\n",
    "                subset = dataset[pack*50000:min(numSample, (pack+1)*50000)]\n",
    "                matrix2 = subset.transpose()\n",
    "                matrix3 = get_xy_square(subset, centroids, k)\n",
    "                \n",
    "                distance_mat = np.hstack((distance_mat, dis_computation(matrix1, matrix2, matrix3)))\n",
    "\n",
    "        else:\n",
    "            matrix1 = centroids\n",
    "            matrix2 = dataset.transpose()\n",
    "            matrix3 = get_xy_square(dataset, centroids, k)\n",
    "            distance_matrix = dis_computation(matrix1, matrix2, matrix3)\n",
    "\n",
    "        clusterAssment = cluster_assignment_new(distance_mat, k)\n",
    "\n",
    "\n",
    "## step4 循环结束后更新聚类中心\n",
    "        #start = time.time()\n",
    "        #index, cluster, count_dict = sort_by_key(clusterAssment)\n",
    "        #index, cluster = sort_by_key_without_count(clusterAssment)\n",
    "        #collection = []\n",
    "        for i in range(k):\n",
    "            comp = np.nonzero(clusterAssment[:,0] == i)[0] # 当前状态的聚类情况\n",
    "            comp1 = np.nonzero(clusterAssment1[:,0] == i)[0] # 上一状态的聚类情况\n",
    "            if(len(comp) == len(comp1) and (comp == comp1).all()):\n",
    "                 # 当前簇内元素没有改变？如果没有改变，下次聚类忽略该簇\n",
    "                continue\n",
    "                \n",
    "            #pointsInCluster = sort_gpu(index, cluster, dataset, k, i) \n",
    "            points_In_k_Cluster_Label=np.nonzero(clusterAssment[:,0]==i)[0]\n",
    "            pointsInCluster=dataset[points_In_k_Cluster_Label]\n",
    "            centroids[i, :] = np.mean(pointsInCluster, axis=0)\n",
    "            #centroids[i,:] = centroids_update(pointsInCluster, k)\n",
    "        #end = time.time()\n",
    "        #t4centupdate += (end-start)\n",
    "    #print(t4centupdate)\n",
    "        if(itr == 50):            \n",
    "            break\n",
    "    print(itr)\n",
    "    \n",
    "    ##循环结束，返回聚类中心和标签信息\n",
    "    print(\"Congratulations, cluster complete!\")\n",
    "    return centroids,clusterAssment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "Congratulations, cluster complete!\n",
      "algorithm (for training) total time: 18.131549 秒\n",
      "Counter({5.0: 280790, 4.0: 107201, 0.0: 97278, 13.0: 2203, 15.0: 1589, 10.0: 1247, 9.0: 1040, 20.0: 1020, 8.0: 979, 7.0: 264, 17.0: 231, 6.0: 53, 1.0: 30, 11.0: 21, 19.0: 20, 14.0: 12, 22.0: 10, 2.0: 9, 12.0: 8, 18.0: 7, 16.0: 4, 3.0: 3, 21.0: 2})\n",
      "Counter({19: 261464, 11: 48370, 8: 28782, 13: 28625, 3: 22731, 10: 19850, 6: 17784, 12: 11372, 9: 10354, 7: 9977, 18: 7732, 1: 5871, 0: 5592, 2: 5458, 21: 4222, 15: 1962, 14: 1516, 16: 1145, 5: 1068, 20: 48, 4: 43, 17: 38, 22: 17})\n",
      "调整兰德指数为0.7671254747783168\n",
      "归一化互信息指数为0.6889604931693692\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    # start=time.time()\n",
    "    ## load data\n",
    "    dataset=pd.read_csv('kdd_pre_corr.csv',sep=',')\n",
    "    # 真实的标签\n",
    "    category_real = dataset.loc[:,[\"classification\"]]\n",
    " \n",
    "    dataset=dataset.loc[:,['duration','src_bytes','dst_types','land','wrong_fragment','urgent',\n",
    "                           'hot','num_falied_logins','logged_in','num_compromised','root_shell','su_attempted','num_root',\n",
    "                           'num_file_creations','num_shells','num_access_files','num_outbound_cmds','is_hot_login','is_guest_login',\n",
    "                           'count','srv_count','serror_rate','srv_serror_rate','rerror_rate','srv_rerror_rate','same_error_rate',\n",
    "                           'diff_error_rate','srv_diff_host_rate','dst_host_count','dst_host_srv_count','dst_host_same_srv_rate',\n",
    "                           'dst_host_diff_srv_rate','dst_host_same_src_port_rate','dst_host_diff_src_port_rate',\n",
    "                           'dst_host_serror_rate','dst_host_srv_serror_rate','dst_host_rerror_rate','dst_host_srv_rerror_rate']]\n",
    "\n",
    "    \n",
    "    dataset=np.array(dataset)\n",
    "    \n",
    "    ##  k表示聚类中心数\n",
    "    k = 23\n",
    "    for _ in range(1):\n",
    "        start=time.time()\n",
    "        centroids,clusterAssment=kmeans(dataset,k)\n",
    "\n",
    "        end = time.time()\n",
    "        print('algorithm (for training) total time: %2f 秒'%(end-start))\n",
    "    \n",
    "    category_real = np.array(category_real)\n",
    "    category = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        category.append(category_real[i][0])\n",
    "    category = np.array(category)\n",
    "    category_pre = np.array(clusterAssment[:,0], dtype = np.int32)\n",
    "    real = Counter(category)\n",
    "    pre = Counter(category_pre)\n",
    "    print(real)\n",
    "    print(pre)\n",
    "    real = real.most_common()\n",
    "    pre = pre.most_common()\n",
    "    for j in range(dataset.shape[0]):\n",
    "        for nn in range(k):\n",
    "            if(category[j] == real[nn][0]):\n",
    "                category[j] = int(pre[nn][0])\n",
    "    ARI = metrics.adjusted_rand_score(category, category_pre)\n",
    "    AMI = metrics.adjusted_mutual_info_score(category, category_pre)\n",
    "    print(\"调整兰德指数为\" + str(ARI))\n",
    "    print(\"归一化互信息指数为\" + str(AMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
